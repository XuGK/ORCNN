{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You may need to restart your runtime prior to this, to let your installation take effect\n",
    "# Some basic setup:\n",
    "# Setup detectron2 logger\n",
    "import detectron2\n",
    "from detectron2.utils.logger import setup_logger\n",
    "setup_logger()\n",
    "\n",
    "# import some common libraries\n",
    "import numpy as np\n",
    "import cv2\n",
    "import random\n",
    "# from google.colab.patches import cv2_imshow\n",
    "\n",
    "# import some common detectron2 utilities\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import MetadataCatalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pylab\n",
    "pylab.rcParams['figure.figsize'] = 13,13\n",
    "def imshow(img):\n",
    "    plt.imshow(img[:, :, [2, 1, 0]])\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run a pre-trained detectron2 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget http://images.cocodataset.org/val2017/000000439715.jpg -O input.jpg\n",
    "im = cv2.imread(\"./input.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = get_cfg()\n",
    "# add project-specific config (e.g., TensorMask) here if you're not running a model in detectron2's core library\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # set threshold for this model\n",
    "# Find a model from detectron2's model zoo. You can use the https://dl.fbaipublicfiles... url as well\n",
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")\n",
    "cfg.DATASETS.TRAIN = (\"amodal_train\",)\n",
    "cfg.DATASETS.TEST = (\"small_amodal_test\")\n",
    "predictor = DefaultPredictor(cfg)\n",
    "outputs = predictor(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the outputs. See https://detectron2.readthedocs.io/tutorials/models.html#model-output-format for specification\n",
    "outputs[\"instances\"].pred_classes\n",
    "outputs[\"instances\"].pred_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use `Visualizer` to draw the predictions on the image.\n",
    "v = Visualizer(im[:, :, ::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.2)\n",
    "v = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow(v.get_image()[:, :, ::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.engine import DefaultTrainer\n",
    "trainer = DefaultTrainer(cfg) \n",
    "trainer.resume_or_load(resume=False)\n",
    "#  evaluate its performance using AP metric implemented in COCO API.\n",
    "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
    "from detectron2.data import build_detection_test_loader\n",
    "evaluator = COCOEvaluator(\"amodal_val\", cfg, False, output_dir=\"./output/\")\n",
    "val_loader = build_detection_test_loader(cfg, \"amodal_val\")\n",
    "inference_on_dataset(trainer.model, val_loader, evaluator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train pre-trained model on a custom dataset (FT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[02/28 23:05:57 d2.data.datasets.coco]: \u001b[0mLoaded 2276 images in COCO format from datasets/coco/annotations/COCO_amodal_train2014_with_classes_poly.json\n"
     ]
    }
   ],
   "source": [
    "# Register my amodal datasets \n",
    "from detectron2.data.datasets import register_coco_instances\n",
    "from detectron2.data import MetadataCatalog\n",
    "register_coco_instances(\"amodal_coco_train\", {}, \"datasets/coco/annotations/COCO_amodal_train2014_with_classes_poly.json\", \"datasets/coco/train2014\")\n",
    "# Prepare test datasets \n",
    "from detectron2.data.datasets import register_coco_instances\n",
    "from detectron2.data import MetadataCatalog\n",
    "register_coco_instances(\"amodal_coco_val\", {}, \"datasets/coco/annotations/COCO_amodal_val2014_with_classes_poly.json\", \"datasets/coco/val2014\")\n",
    "from detectron2.data import DatasetCatalog\n",
    "dataset_dicts = DatasetCatalog.get(\"amodal_coco_train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register small datasets for debugging \n",
    "from detectron2.data import DatasetCatalog,MetadataCatalog\n",
    "dataset_dicts = DatasetCatalog.get(\"amodal_train\")\n",
    "import random\n",
    "DatasetCatalog.register(\"small_amodal_test\", lambda : random.sample(dataset_dicts,k=4))\n",
    "metadata = {}\n",
    "MetadataCatalog.get(\"small_amodal_test\").set(\n",
    "        json_file=\"datasets/coco/annotations/COCO_amodal_train2014_with_classes_poly.json\", image_root=\"datasets/coco/train2014\", evaluator_type=\"coco\", **metadata\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset_dicts[3]['annotations'][4]['invisible_mask'])\n",
    "print(dataset_dicts[3]['annotations'][2]['visible_mask'])\n",
    "print(dataset_dicts[3]['annotations'][2]['segmentation'])\n",
    "MetadataCatalog.get(\"amodal_train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Run pre-trained model on this amodal image \n",
    "cfg = get_cfg()\n",
    "# add project-specific config (e.g., TensorMask) here if you're not running a model in detectron2's core library\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # set threshold for this model\n",
    "# Find a model from detectron2's model zoo. You can use the https://dl.fbaipublicfiles... url as well\n",
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")\n",
    "from detectron2.data import DatasetMapper\n",
    "mapper = DatasetMapper(cfg,is_train=True)\n",
    "mydict = mapper(dataset_dicts[3])\n",
    "boxes = mydict['instances'].gt_boxes.tensor\n",
    "print(mydict['instances'].gt_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## To verify the data loading is correct, let's visualize the annotations of randomly selected samples in the training set:\n",
    "import random\n",
    "from detectron2.data import DatasetCatalog\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "dataset_dicts = DatasetCatalog.get(\"amodal_train\")\n",
    "for d in random.sample(dataset_dicts, 1):\n",
    "    img = cv2.imread(d[\"file_name\"])\n",
    "    visualizer = Visualizer(img[:, :, ::-1], metadata=MetadataCatalog.get(\"amodal_train\"), scale=0.5)\n",
    "    vis = visualizer.draw_dataset_dict(d)\n",
    "    imshow(vis.get_image()[:, :, ::-1])\n",
    "    predictor = DefaultPredictor(cfg)\n",
    "    outputs = predictor(img)\n",
    "    # We can use `Visualizer` to draw the predictions on the image.\n",
    "    v = Visualizer(img[:, :, ::-1], MetadataCatalog.get(\"amodal_train\"), scale=1.2)\n",
    "    v = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
    "    imshow(v.get_image()[:, :, ::-1])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[02/28 23:06:18 d2.engine.defaults]: \u001b[0mModel:\n",
      "GeneralizedRCNN(\n",
      "  (backbone): FPN(\n",
      "    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (top_block): LastLevelMaxPool()\n",
      "    (bottom_up): ResNet(\n",
      "      (stem): BasicStem(\n",
      "        (conv1): Conv2d(\n",
      "          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "      (res2): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res3): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (3): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res4): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (3): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (4): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (5): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res5): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (proposal_generator): RPN(\n",
      "    (anchor_generator): DefaultAnchorGenerator(\n",
      "      (cell_anchors): BufferList()\n",
      "    )\n",
      "    (rpn_head): StandardRPNHead(\n",
      "      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "  )\n",
      "  (roi_heads): StandardROIHeads(\n",
      "    (box_pooler): ROIPooler(\n",
      "      (level_poolers): ModuleList(\n",
      "        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
      "        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
      "        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
      "        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
      "      )\n",
      "    )\n",
      "    (box_head): FastRCNNConvFCHead(\n",
      "      (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n",
      "      (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    )\n",
      "    (box_predictor): FastRCNNOutputLayers(\n",
      "      (cls_score): Linear(in_features=1024, out_features=81, bias=True)\n",
      "      (bbox_pred): Linear(in_features=1024, out_features=320, bias=True)\n",
      "    )\n",
      "    (mask_pooler): ROIPooler(\n",
      "      (level_poolers): ModuleList(\n",
      "        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
      "        (1): ROIAlign(output_size=(14, 14), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
      "        (2): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
      "        (3): ROIAlign(output_size=(14, 14), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
      "      )\n",
      "    )\n",
      "    (mask_head): MaskRCNNConvUpsampleHead(\n",
      "      (mask_fcn1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (mask_fcn2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (mask_fcn3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (mask_fcn4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (deconv): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n",
      "      (predictor): Conv2d(256, 80, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[02/28 23:06:18 d2.data.datasets.coco]: \u001b[0mLoaded 2276 images in COCO format from datasets/coco/annotations/COCO_amodal_train2014_with_classes_poly.json\n",
      "\u001b[32m[02/28 23:06:18 d2.data.build]: \u001b[0mRemoved 0 images with no usable annotations. 2276 images left.\n",
      "\u001b[32m[02/28 23:06:18 d2.data.build]: \u001b[0mDistribution of instances among all 80 categories:\n",
      "\u001b[36m|   category    | #instances   |   category   | #instances   |   category    | #instances   |\n",
      "|:-------------:|:-------------|:------------:|:-------------|:-------------:|:-------------|\n",
      "|    person     | 2311         |   bicycle    | 21           |      car      | 279          |\n",
      "|  motorcycle   | 47           |   airplane   | 65           |      bus      | 92           |\n",
      "|     train     | 60           |    truck     | 115          |     boat      | 79           |\n",
      "| traffic light | 13           | fire hydrant | 35           |   stop sign   | 12           |\n",
      "| parking meter | 10           |    bench     | 48           |     bird      | 57           |\n",
      "|      cat      | 94           |     dog      | 100          |     horse     | 57           |\n",
      "|     sheep     | 102          |     cow      | 91           |   elephant    | 80           |\n",
      "|     bear      | 28           |    zebra     | 84           |    giraffe    | 90           |\n",
      "|   backpack    | 14           |   umbrella   | 74           |    handbag    | 26           |\n",
      "|      tie      | 9            |   suitcase   | 77           |    frisbee    | 29           |\n",
      "|     skis      | 13           |  snowboard   | 21           |  sports ball  | 36           |\n",
      "|     kite      | 49           | baseball bat | 33           | baseball gl.. | 5            |\n",
      "|  skateboard   | 41           |  surfboard   | 37           | tennis racket | 49           |\n",
      "|    bottle     | 222          |  wine glass  | 70           |      cup      | 215          |\n",
      "|     fork      | 25           |    knife     | 69           |     spoon     | 33           |\n",
      "|     bowl      | 161          |    banana    | 26           |     apple     | 34           |\n",
      "|   sandwich    | 33           |    orange    | 31           |   broccoli    | 9            |\n",
      "|    carrot     | 19           |   hot dog    | 27           |     pizza     | 60           |\n",
      "|     donut     | 70           |     cake     | 76           |     chair     | 195          |\n",
      "|     couch     | 55           | potted plant | 12           |      bed      | 26           |\n",
      "| dining table  | 34           |    toilet    | 80           |      tv       | 91           |\n",
      "|    laptop     | 74           |    mouse     | 25           |    remote     | 28           |\n",
      "|   keyboard    | 36           |  cell phone  | 38           |   microwave   | 33           |\n",
      "|     oven      | 23           |   toaster    | 5            |     sink      | 53           |\n",
      "| refrigerator  | 63           |     book     | 59           |     clock     | 39           |\n",
      "|     vase      | 45           |   scissors   | 8            |  teddy bear   | 68           |\n",
      "|  hair drier   | 0            |  toothbrush  | 10           |               |              |\n",
      "|     total     | 6763         |              |              |               |              |\u001b[0m\n",
      "\u001b[32m[02/28 23:06:18 d2.data.detection_utils]: \u001b[0mTransformGens used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]\n",
      "\u001b[32m[02/28 23:06:18 d2.data.build]: \u001b[0mUsing training sampler TrainingSampler\n",
      "\u001b[32m[02/28 23:06:19 d2.engine.train_loop]: \u001b[0mStarting training from iteration 0\n",
      "\u001b[32m[02/28 23:06:24 d2.utils.events]: \u001b[0meta: 0:06:39  iter: 19  total_loss: 0.628  loss_cls: 0.161  loss_box_reg: 0.104  loss_mask: 0.211  loss_rpn_cls: 0.020  loss_rpn_loc: 0.014  time: 0.2663  data_time: 0.0125  lr: 0.000010  max_mem: 2676M\n",
      "\u001b[32m[02/28 23:06:30 d2.utils.events]: \u001b[0meta: 0:06:31  iter: 39  total_loss: 0.498  loss_cls: 0.174  loss_box_reg: 0.092  loss_mask: 0.188  loss_rpn_cls: 0.018  loss_rpn_loc: 0.013  time: 0.2712  data_time: 0.0143  lr: 0.000020  max_mem: 2677M\n",
      "\u001b[32m[02/28 23:06:36 d2.utils.events]: \u001b[0meta: 0:06:24  iter: 59  total_loss: 0.589  loss_cls: 0.132  loss_box_reg: 0.086  loss_mask: 0.304  loss_rpn_cls: 0.023  loss_rpn_loc: 0.009  time: 0.2708  data_time: 0.0120  lr: 0.000030  max_mem: 2764M\n",
      "\u001b[32m[02/28 23:06:42 d2.utils.events]: \u001b[0meta: 0:06:15  iter: 79  total_loss: 0.478  loss_cls: 0.141  loss_box_reg: 0.094  loss_mask: 0.220  loss_rpn_cls: 0.019  loss_rpn_loc: 0.009  time: 0.2714  data_time: 0.0033  lr: 0.000040  max_mem: 2764M\n",
      "\u001b[32m[02/28 23:06:48 d2.utils.events]: \u001b[0meta: 0:06:07  iter: 99  total_loss: 0.516  loss_cls: 0.176  loss_box_reg: 0.065  loss_mask: 0.237  loss_rpn_cls: 0.021  loss_rpn_loc: 0.012  time: 0.2710  data_time: 0.0171  lr: 0.000050  max_mem: 2764M\n",
      "\u001b[32m[02/28 23:06:54 d2.utils.events]: \u001b[0meta: 0:06:01  iter: 119  total_loss: 0.484  loss_cls: 0.149  loss_box_reg: 0.117  loss_mask: 0.189  loss_rpn_cls: 0.015  loss_rpn_loc: 0.014  time: 0.2713  data_time: 0.0130  lr: 0.000060  max_mem: 2819M\n",
      "\u001b[32m[02/28 23:07:00 d2.utils.events]: \u001b[0meta: 0:05:57  iter: 139  total_loss: 0.474  loss_cls: 0.111  loss_box_reg: 0.099  loss_mask: 0.210  loss_rpn_cls: 0.018  loss_rpn_loc: 0.012  time: 0.2726  data_time: 0.0132  lr: 0.000070  max_mem: 2819M\n",
      "\u001b[32m[02/28 23:07:06 d2.utils.events]: \u001b[0meta: 0:05:53  iter: 159  total_loss: 0.490  loss_cls: 0.149  loss_box_reg: 0.104  loss_mask: 0.223  loss_rpn_cls: 0.020  loss_rpn_loc: 0.010  time: 0.2740  data_time: 0.0148  lr: 0.000080  max_mem: 2819M\n",
      "\u001b[32m[02/28 23:07:12 d2.utils.events]: \u001b[0meta: 0:05:48  iter: 179  total_loss: 0.351  loss_cls: 0.106  loss_box_reg: 0.069  loss_mask: 0.156  loss_rpn_cls: 0.006  loss_rpn_loc: 0.006  time: 0.2735  data_time: 0.0111  lr: 0.000090  max_mem: 2819M\n",
      "\u001b[32m[02/28 23:07:17 d2.utils.events]: \u001b[0meta: 0:05:41  iter: 199  total_loss: 0.418  loss_cls: 0.099  loss_box_reg: 0.096  loss_mask: 0.191  loss_rpn_cls: 0.013  loss_rpn_loc: 0.009  time: 0.2729  data_time: 0.0110  lr: 0.000100  max_mem: 2819M\n",
      "\u001b[32m[02/28 23:07:23 d2.utils.events]: \u001b[0meta: 0:05:36  iter: 219  total_loss: 0.514  loss_cls: 0.125  loss_box_reg: 0.102  loss_mask: 0.256  loss_rpn_cls: 0.013  loss_rpn_loc: 0.008  time: 0.2728  data_time: 0.0140  lr: 0.000110  max_mem: 2819M\n",
      "\u001b[32m[02/28 23:07:29 d2.utils.events]: \u001b[0meta: 0:05:31  iter: 239  total_loss: 0.459  loss_cls: 0.102  loss_box_reg: 0.086  loss_mask: 0.176  loss_rpn_cls: 0.011  loss_rpn_loc: 0.012  time: 0.2725  data_time: 0.0112  lr: 0.000120  max_mem: 2819M\n",
      "\u001b[32m[02/28 23:07:35 d2.utils.events]: \u001b[0meta: 0:05:25  iter: 259  total_loss: 0.423  loss_cls: 0.109  loss_box_reg: 0.078  loss_mask: 0.190  loss_rpn_cls: 0.013  loss_rpn_loc: 0.014  time: 0.2726  data_time: 0.0142  lr: 0.000130  max_mem: 2819M\n",
      "\u001b[32m[02/28 23:07:41 d2.utils.events]: \u001b[0meta: 0:05:20  iter: 279  total_loss: 0.471  loss_cls: 0.131  loss_box_reg: 0.103  loss_mask: 0.220  loss_rpn_cls: 0.023  loss_rpn_loc: 0.017  time: 0.2734  data_time: 0.0120  lr: 0.000140  max_mem: 2922M\n",
      "\u001b[32m[02/28 23:07:47 d2.utils.events]: \u001b[0meta: 0:05:15  iter: 299  total_loss: 0.505  loss_cls: 0.124  loss_box_reg: 0.101  loss_mask: 0.190  loss_rpn_cls: 0.008  loss_rpn_loc: 0.013  time: 0.2738  data_time: 0.0150  lr: 0.000150  max_mem: 2922M\n",
      "\u001b[32m[02/28 23:07:53 d2.utils.events]: \u001b[0meta: 0:05:09  iter: 319  total_loss: 0.371  loss_cls: 0.088  loss_box_reg: 0.069  loss_mask: 0.171  loss_rpn_cls: 0.012  loss_rpn_loc: 0.007  time: 0.2737  data_time: 0.0140  lr: 0.000160  max_mem: 2922M\n",
      "\u001b[32m[02/28 23:07:59 d2.utils.events]: \u001b[0meta: 0:05:05  iter: 339  total_loss: 0.366  loss_cls: 0.093  loss_box_reg: 0.071  loss_mask: 0.147  loss_rpn_cls: 0.009  loss_rpn_loc: 0.009  time: 0.2741  data_time: 0.0109  lr: 0.000170  max_mem: 2922M\n",
      "\u001b[32m[02/28 23:08:05 d2.utils.events]: \u001b[0meta: 0:05:00  iter: 359  total_loss: 0.560  loss_cls: 0.118  loss_box_reg: 0.107  loss_mask: 0.276  loss_rpn_cls: 0.009  loss_rpn_loc: 0.008  time: 0.2742  data_time: 0.0147  lr: 0.000180  max_mem: 2922M\n",
      "\u001b[32m[02/28 23:08:11 d2.utils.events]: \u001b[0meta: 0:04:55  iter: 379  total_loss: 0.444  loss_cls: 0.113  loss_box_reg: 0.075  loss_mask: 0.192  loss_rpn_cls: 0.015  loss_rpn_loc: 0.011  time: 0.2747  data_time: 0.0117  lr: 0.000190  max_mem: 2922M\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[02/28 23:08:17 d2.utils.events]: \u001b[0meta: 0:04:50  iter: 399  total_loss: 0.533  loss_cls: 0.123  loss_box_reg: 0.125  loss_mask: 0.221  loss_rpn_cls: 0.010  loss_rpn_loc: 0.010  time: 0.2752  data_time: 0.0145  lr: 0.000200  max_mem: 2922M\n",
      "\u001b[32m[02/28 23:08:23 d2.utils.events]: \u001b[0meta: 0:04:45  iter: 419  total_loss: 0.428  loss_cls: 0.089  loss_box_reg: 0.091  loss_mask: 0.208  loss_rpn_cls: 0.008  loss_rpn_loc: 0.013  time: 0.2755  data_time: 0.0192  lr: 0.000210  max_mem: 2922M\n",
      "\u001b[32m[02/28 23:08:30 d2.utils.events]: \u001b[0meta: 0:04:40  iter: 439  total_loss: 0.460  loss_cls: 0.109  loss_box_reg: 0.096  loss_mask: 0.211  loss_rpn_cls: 0.009  loss_rpn_loc: 0.011  time: 0.2761  data_time: 0.0135  lr: 0.000220  max_mem: 2922M\n",
      "\u001b[32m[02/28 23:08:35 d2.utils.events]: \u001b[0meta: 0:04:34  iter: 459  total_loss: 0.347  loss_cls: 0.095  loss_box_reg: 0.073  loss_mask: 0.157  loss_rpn_cls: 0.006  loss_rpn_loc: 0.008  time: 0.2761  data_time: 0.0119  lr: 0.000230  max_mem: 2922M\n",
      "\u001b[32m[02/28 23:08:42 d2.utils.events]: \u001b[0meta: 0:04:29  iter: 479  total_loss: 0.430  loss_cls: 0.121  loss_box_reg: 0.083  loss_mask: 0.167  loss_rpn_cls: 0.007  loss_rpn_loc: 0.012  time: 0.2763  data_time: 0.0120  lr: 0.000240  max_mem: 2922M\n",
      "\u001b[32m[02/28 23:08:48 d2.utils.events]: \u001b[0meta: 0:04:25  iter: 499  total_loss: 0.616  loss_cls: 0.145  loss_box_reg: 0.156  loss_mask: 0.255  loss_rpn_cls: 0.011  loss_rpn_loc: 0.016  time: 0.2774  data_time: 0.0122  lr: 0.000250  max_mem: 2954M\n",
      "\u001b[32m[02/28 23:08:54 d2.utils.events]: \u001b[0meta: 0:04:20  iter: 519  total_loss: 0.435  loss_cls: 0.106  loss_box_reg: 0.109  loss_mask: 0.185  loss_rpn_cls: 0.010  loss_rpn_loc: 0.015  time: 0.2774  data_time: 0.0172  lr: 0.000260  max_mem: 2954M\n",
      "\u001b[32m[02/28 23:09:00 d2.utils.events]: \u001b[0meta: 0:04:15  iter: 539  total_loss: 0.378  loss_cls: 0.104  loss_box_reg: 0.099  loss_mask: 0.174  loss_rpn_cls: 0.011  loss_rpn_loc: 0.012  time: 0.2778  data_time: 0.0114  lr: 0.000270  max_mem: 2954M\n",
      "\u001b[32m[02/28 23:09:06 d2.utils.events]: \u001b[0meta: 0:04:10  iter: 559  total_loss: 0.392  loss_cls: 0.094  loss_box_reg: 0.070  loss_mask: 0.199  loss_rpn_cls: 0.011  loss_rpn_loc: 0.010  time: 0.2778  data_time: 0.0146  lr: 0.000280  max_mem: 2954M\n",
      "\u001b[32m[02/28 23:09:13 d2.utils.events]: \u001b[0meta: 0:04:04  iter: 579  total_loss: 0.470  loss_cls: 0.103  loss_box_reg: 0.101  loss_mask: 0.174  loss_rpn_cls: 0.011  loss_rpn_loc: 0.012  time: 0.2781  data_time: 0.0182  lr: 0.000290  max_mem: 2954M\n",
      "\u001b[32m[02/28 23:09:19 d2.utils.events]: \u001b[0meta: 0:03:59  iter: 599  total_loss: 0.441  loss_cls: 0.122  loss_box_reg: 0.109  loss_mask: 0.187  loss_rpn_cls: 0.009  loss_rpn_loc: 0.011  time: 0.2780  data_time: 0.0119  lr: 0.000300  max_mem: 2954M\n",
      "\u001b[32m[02/28 23:09:25 d2.utils.events]: \u001b[0meta: 0:03:54  iter: 619  total_loss: 0.384  loss_cls: 0.089  loss_box_reg: 0.084  loss_mask: 0.207  loss_rpn_cls: 0.006  loss_rpn_loc: 0.009  time: 0.2781  data_time: 0.0034  lr: 0.000310  max_mem: 2954M\n",
      "\u001b[32m[02/28 23:09:31 d2.utils.events]: \u001b[0meta: 0:03:49  iter: 639  total_loss: 0.467  loss_cls: 0.124  loss_box_reg: 0.120  loss_mask: 0.191  loss_rpn_cls: 0.008  loss_rpn_loc: 0.012  time: 0.2784  data_time: 0.0098  lr: 0.000320  max_mem: 2954M\n",
      "\u001b[32m[02/28 23:09:37 d2.utils.events]: \u001b[0meta: 0:03:44  iter: 659  total_loss: 0.484  loss_cls: 0.117  loss_box_reg: 0.113  loss_mask: 0.225  loss_rpn_cls: 0.011  loss_rpn_loc: 0.011  time: 0.2789  data_time: 0.0033  lr: 0.000330  max_mem: 2954M\n",
      "\u001b[32m[02/28 23:09:43 d2.utils.events]: \u001b[0meta: 0:03:38  iter: 679  total_loss: 0.421  loss_cls: 0.121  loss_box_reg: 0.103  loss_mask: 0.180  loss_rpn_cls: 0.012  loss_rpn_loc: 0.011  time: 0.2788  data_time: 0.0152  lr: 0.000340  max_mem: 2954M\n",
      "\u001b[32m[02/28 23:09:49 d2.utils.events]: \u001b[0meta: 0:03:33  iter: 699  total_loss: 0.410  loss_cls: 0.101  loss_box_reg: 0.083  loss_mask: 0.171  loss_rpn_cls: 0.007  loss_rpn_loc: 0.015  time: 0.2789  data_time: 0.0110  lr: 0.000350  max_mem: 2954M\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from detectron2.engine import DefaultTrainer\n",
    "from detectron2.config import get_cfg\n",
    "\n",
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\n",
    "cfg.DATASETS.TRAIN = (\"amodal_coco_train\",)\n",
    "cfg.DATASETS.TEST = (\"amodal_coco_train\",)\n",
    "cfg.DATALOADER.NUM_WORKERS = 2\n",
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")  # Let training initialize from model zoo\n",
    "cfg.SOLVER.IMS_PER_BATCH = 2\n",
    "cfg.SOLVER.BASE_LR = 0.0005  # pick a good LR\n",
    "cfg.SOLVER.STEPS = (1200,1300)\n",
    "cfg.SOLVER.MAX_ITER = 1500\n",
    "cfg.VIS_PERIOD = 20\n",
    "cfg.OUTPUT_DIR = \"myAmodalCheckpoint\"\n",
    "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
    "trainer = DefaultTrainer(cfg) \n",
    "trainer.resume_or_load(resume=False)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tensorboard --logdir=myAmodalCheckpoint --port=6006"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Inference & evaluation using the trained model\n",
    "Now, let's run inference with the trained model on the balloon validation dataset. First, let's create a predictor using the model we just trained:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "cfg.DATASETS.TEST = (\"amodal_coco_val\",)\n",
    "cfg.OUTPUT_DIR = \"myAmodalCheckpoint\"\n",
    "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\n",
    "# cfg.MODEL.WEIGHTS =  model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")  # Let training initialize from model zoo\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.9   # set the testing threshold for this model\n",
    "predictor = DefaultPredictor(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  evaluate its performance using AP metric implemented in COCO API.\n",
    "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
    "from detectron2.data import build_detection_test_loader\n",
    "evaluator = COCOEvaluator(\"amodal_coco_val\", cfg, False, output_dir=\"myAmodalEvaluation\")\n",
    "val_loader = build_detection_test_loader(cfg, \"amodal_coco_val\")\n",
    "inference_on_dataset(trainer.model, val_loader, evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.utils.visualizer import ColorMode\n",
    "import random\n",
    "from detectron2.data import DatasetCatalog\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "\n",
    "dataset_dicts = DatasetCatalog.get(\"amodal_coco_val\")\n",
    "for d in random.sample(dataset_dicts, 2):    \n",
    "    im = cv2.imread(d[\"file_name\"])\n",
    "    outputs = predictor(im)\n",
    "    v = Visualizer(im[:, :, ::-1],\n",
    "                   metadata=MetadataCatalog.get(\"amodal_coco_val\"), \n",
    "                   scale=0.8, \n",
    "                   instance_mode=ColorMode.IMAGE_BW   # remove the colors of unsegmented pixels\n",
    "    )\n",
    "    v = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
    "    imshow(v.get_image()[:, :, ::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
