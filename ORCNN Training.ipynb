{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You may need to restart your runtime prior to this, to let your installation take effect\n",
    "# Some basic setup:\n",
    "# Setup detectron2 logger\n",
    "import detectron2\n",
    "from detectron2.utils.logger import setup_logger\n",
    "setup_logger()\n",
    "\n",
    "# import some common libraries\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import random\n",
    "# from google.colab.patches import cv2_imshow\n",
    "\n",
    "# import some common detectron2 utilities\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import MetadataCatalog\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pylab\n",
    "pylab.rcParams['figure.figsize'] = 10,10\n",
    "def imshow(img):\n",
    "    plt.imshow(img[:, :, [2, 1, 0]])\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train ORCNN on amodal datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register my amodal datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.data.datasets import register_coco_instances\n",
    "from detectron2.data import MetadataCatalog\n",
    "dataDir='datasets/coco'\n",
    "annTrainFile='{}/annotations/COCO_amodal_train2014_with_classes_poly.json'.format(dataDir)\n",
    "imgTrainFile = '{}/train2014'.format(dataDir)\n",
    "register_coco_instances(\"amodal_coco_train\", {},annTrainFile , imgTrainFile)\n",
    "# Prepare test datasets \n",
    "annTestFile='{}/annotations/COCO_amodal_val2014_with_classes_poly.json'.format(dataDir)\n",
    "imgTestFile = '{}/val2014'.format(dataDir)\n",
    "from detectron2.data.datasets import register_coco_instances\n",
    "from detectron2.data import MetadataCatalog\n",
    "register_coco_instances(\"amodal_coco_val\", {}, annTestFile, imgTestFile)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity checks : overfitting small datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register small datasets for debugging \n",
    "from detectron2.data import DatasetCatalog,MetadataCatalog\n",
    "dataset_dicts = DatasetCatalog.get(\"amodal_coco_train\")\n",
    "import random\n",
    "imgs = random.sample(dataset_dicts,k=2)\n",
    "DatasetCatalog.register(\"small_amodal_test\", lambda : imgs)\n",
    "metadata = {}\n",
    "MetadataCatalog.get(\"small_amodal_test\").set(\n",
    "        image_root=\"datasets/coco/train2014\", evaluator_type=\"coco\", **metadata\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pycocotools.coco import COCO\n",
    "# coco_api = COCO(annTrainFile)\n",
    "# smalldicts = DatasetCatalog.get(\"small_amodal_test\")\n",
    "# cat_ids = sorted(coco_api.getCatIds())\n",
    "# cats = coco_api.loadCats(cat_ids)\n",
    "# # The categories in a custom json file may not be sorted.\n",
    "# tc = [c[\"name\"] for c in sorted(cats, key=lambda x: x[\"id\"])]\n",
    "# print(tc)\n",
    "# my_tc = []\n",
    "# for i in range(len(smalldicts)):\n",
    "#     for j in range(len(smalldicts[i]['annotations'])):\n",
    "#         my_tc.append(smalldicts[i]['annotations'][j]['category_id'])\n",
    "# my_tc = list( dict.fromkeys(my_tc) )\n",
    "# my_tc = sorted(my_tc)\n",
    "# print(my_tc)\n",
    "# thing_classes = [tc[i] for i in my_tc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycocotools.coco import COCO\n",
    "coco_api = COCO(annTrainFile)\n",
    "cat_ids = sorted(coco_api.getCatIds())\n",
    "cats = coco_api.loadCats(cat_ids)\n",
    "# The categories in a custom json file may not be sorted.\n",
    "thing_classes = [c[\"name\"] for c in sorted(cats, key=lambda x: x[\"id\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = {}\n",
    "MetadataCatalog.get(\"small_amodal_test\").set(thing_classes = thing_classes, **metadata )\n",
    "MetadataCatalog.get(\"small_amodal_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smalldicts = DatasetCatalog.get(\"small_amodal_test\")\n",
    "for i in range(len(smalldicts)):\n",
    "    im = cv2.imread(smalldicts[i][\"file_name\"])\n",
    "    imshow(im[:, :, ::-1])\n",
    "    visualizer = Visualizer(im[:, :, ::-1], metadata=MetadataCatalog.get(\"small_amodal_test\"), scale=0.5)\n",
    "    vis = visualizer.draw_dataset_dict(smalldicts[i])\n",
    "    imshow(vis.get_image()[:, :, ::-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from detectron2.engine import DefaultTrainer\n",
    "from detectron2.config import get_cfg\n",
    "\n",
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_orcnn_R_50_FPN_3x.yaml\"))\n",
    "cfg.DATASETS.TRAIN = (\"small_amodal_test\",)\n",
    "cfg.DATASETS.TEST = (\"small_amodal_test\",)\n",
    "cfg.DATALOADER.NUM_WORKERS = 2\n",
    "# cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")  # Let training initialize from model zoo\n",
    "cfg.SOLVER.IMS_PER_BATCH = 2\n",
    "cfg.SOLVER.BASE_LR = 0.0025  # pick a good LR\n",
    "cfg.SOLVER.STEPS = (1200,1400)\n",
    "cfg.SOLVER.MAX_ITER = 1500 \n",
    "cfg.VIS_PERIOD = 20\n",
    "cfg.OUTPUT_DIR = \"myAmodalCheckpoint\"\n",
    "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.9  # set the testing threshold for this model\n",
    "trainer = DefaultTrainer(cfg) \n",
    "trainer.resume_or_load(resume=False)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tensorboard --logdir=myAmodalCheckpoint --port=6006"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.9   # set the testing threshold for this model\n",
    "#  evaluate its performance using AP metric implemented in COCO API.\n",
    "from detectron2.evaluation import AmodalEvaluator, inference_on_dataset\n",
    "from detectron2.data import build_detection_test_loader\n",
    "evaluator = AmodalEvaluator(\"small_amodal_test\", cfg, False, output_dir=\"myAmodalEvaluation\")\n",
    "val_loader = build_detection_test_loader(cfg, \"small_amodal_test\")\n",
    "# import pdb;pdb.set_trace()\n",
    "inference_on_dataset(trainer.model, val_loader, evaluator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.utils.visualizer import ColorMode\n",
    "import random\n",
    "from detectron2.data import DatasetCatalog\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.9   # set the testing threshold for this model\n",
    "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\n",
    "cfg.DATASETS.TEST = (\"small_amodal_test\",)\n",
    "predictor = DefaultPredictor(cfg)\n",
    "dataset_dicts = DatasetCatalog.get(\"small_amodal_test\")\n",
    "for d in random.sample(dataset_dicts, 2):    \n",
    "    im = cv2.imread(d[\"file_name\"])\n",
    "    visualizer = Visualizer(im[:, :, ::-1], metadata=MetadataCatalog.get(\"small_amodal_test\"), scale=0.5)\n",
    "    vis = visualizer.draw_dataset_dict(d)\n",
    "    imshow(vis.get_image()[:, :, ::-1])\n",
    "#     import pdb;pdb.set_trace()\n",
    "    outputs = predictor(im)\n",
    "    v = Visualizer(im[:, :, ::-1],\n",
    "                   metadata=MetadataCatalog.get(\"small_amodal_test\"), \n",
    "                   scale=0.8, \n",
    "                   instance_mode=ColorMode.IMAGE_BW   # remove the colors of unsegmented pixels\n",
    "    )\n",
    "    v = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
    "    imshow(v.get_image()[:, :, ::-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training from a COCO-pretrained model as provided by Detectron2\n",
    "1. Finetuning : For the case of COCOA amodal the final output layers that are class-specific had to be initialized randomly as the number of classes and their semantic meaning did not fit to the number of classes of COCO."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[02/28 22:50:41 d2.engine.defaults]: \u001b[0mModel:\n",
      "GeneralizedRCNN(\n",
      "  (backbone): FPN(\n",
      "    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (top_block): LastLevelMaxPool()\n",
      "    (bottom_up): ResNet(\n",
      "      (stem): BasicStem(\n",
      "        (conv1): Conv2d(\n",
      "          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "      (res2): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res3): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (3): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res4): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (3): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (4): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (5): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res5): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (proposal_generator): RPN(\n",
      "    (anchor_generator): DefaultAnchorGenerator(\n",
      "      (cell_anchors): BufferList()\n",
      "    )\n",
      "    (rpn_head): StandardRPNHead(\n",
      "      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "  )\n",
      "  (roi_heads): AmodalROIHeads(\n",
      "    (box_pooler): ROIPooler(\n",
      "      (level_poolers): ModuleList(\n",
      "        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
      "        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
      "        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
      "        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
      "      )\n",
      "    )\n",
      "    (box_head): FastRCNNConvFCHead(\n",
      "      (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n",
      "      (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    )\n",
      "    (box_predictor): FastRCNNOutputLayers(\n",
      "      (cls_score): Linear(in_features=1024, out_features=81, bias=True)\n",
      "      (bbox_pred): Linear(in_features=1024, out_features=320, bias=True)\n",
      "    )\n",
      "    (mask_pooler): ROIPooler(\n",
      "      (level_poolers): ModuleList(\n",
      "        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
      "        (1): ROIAlign(output_size=(14, 14), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
      "        (2): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
      "        (3): ROIAlign(output_size=(14, 14), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
      "      )\n",
      "    )\n",
      "    (amodal_mask_head): AmodalMaskRCNNConvUpsampleHead(\n",
      "      (amodal_mask_fcn1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (amodal_mask_fcn2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (amodal_mask_fcn3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (amodal_mask_fcn4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (deconv): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n",
      "      (predictor): Conv2d(256, 80, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (visible_mask_head): VisibleMaskRCNNConvUpsampleHead(\n",
      "      (visible_mask_fcn1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (visible_mask_fcn2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (visible_mask_fcn3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (visible_mask_fcn4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (deconv): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n",
      "      (predictor): Conv2d(256, 80, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (invisible_mask_head): InvisibleMaskRCNNHead()\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[02/28 22:50:42 d2.data.datasets.coco]: \u001b[0mLoaded 2276 images in COCO format from datasets/coco/annotations/COCO_amodal_train2014_with_classes_poly.json\n",
      "\u001b[32m[02/28 22:50:42 d2.data.build]: \u001b[0mRemoved 0 images with no usable annotations. 2276 images left.\n",
      "\u001b[32m[02/28 22:50:42 d2.data.detection_utils]: \u001b[0mTransformGens used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]\n",
      "\u001b[32m[02/28 22:50:42 d2.data.build]: \u001b[0mUsing training sampler TrainingSampler\n",
      "\u001b[32m[02/28 22:50:43 d2.engine.train_loop]: \u001b[0mStarting training from iteration 0\n",
      "\u001b[32m[02/28 22:50:49 d2.utils.events]: \u001b[0meta: 0:06:41  iter: 19  total_loss: 1.066  loss_cls: 0.141  loss_box_reg: 0.125  loss_amodal_mask: 0.196  loss_visible_mask: 0.149  loss_invisible_mask: 0.477  loss_rpn_cls: 0.018  loss_rpn_loc: 0.006  time: 0.2754  data_time: 0.0134  lr: 0.000010  max_mem: 3382M\n",
      "\u001b[32m[02/28 22:50:55 d2.utils.events]: \u001b[0meta: 0:06:48  iter: 39  total_loss: 1.141  loss_cls: 0.173  loss_box_reg: 0.158  loss_amodal_mask: 0.222  loss_visible_mask: 0.170  loss_invisible_mask: 0.380  loss_rpn_cls: 0.019  loss_rpn_loc: 0.013  time: 0.2852  data_time: 0.0123  lr: 0.000020  max_mem: 3382M\n",
      "\u001b[32m[02/28 22:51:01 d2.utils.events]: \u001b[0meta: 0:06:37  iter: 59  total_loss: 1.018  loss_cls: 0.183  loss_box_reg: 0.123  loss_amodal_mask: 0.209  loss_visible_mask: 0.172  loss_invisible_mask: 0.236  loss_rpn_cls: 0.018  loss_rpn_loc: 0.007  time: 0.2830  data_time: 0.0037  lr: 0.000030  max_mem: 3390M\n",
      "\u001b[32m[02/28 22:51:06 d2.utils.events]: \u001b[0meta: 0:06:36  iter: 79  total_loss: 1.187  loss_cls: 0.234  loss_box_reg: 0.205  loss_amodal_mask: 0.246  loss_visible_mask: 0.181  loss_invisible_mask: 0.244  loss_rpn_cls: 0.023  loss_rpn_loc: 0.011  time: 0.2847  data_time: 0.0036  lr: 0.000040  max_mem: 3438M\n",
      "\u001b[32m[02/28 22:51:12 d2.utils.events]: \u001b[0meta: 0:06:30  iter: 99  total_loss: 0.977  loss_cls: 0.157  loss_box_reg: 0.146  loss_amodal_mask: 0.208  loss_visible_mask: 0.167  loss_invisible_mask: 0.170  loss_rpn_cls: 0.016  loss_rpn_loc: 0.010  time: 0.2842  data_time: 0.0035  lr: 0.000050  max_mem: 3545M\n",
      "\u001b[32m[02/28 22:51:18 d2.utils.events]: \u001b[0meta: 0:06:28  iter: 119  total_loss: 0.982  loss_cls: 0.195  loss_box_reg: 0.212  loss_amodal_mask: 0.244  loss_visible_mask: 0.154  loss_invisible_mask: 0.209  loss_rpn_cls: 0.015  loss_rpn_loc: 0.013  time: 0.2863  data_time: 0.0037  lr: 0.000060  max_mem: 3545M\n",
      "\u001b[32m[02/28 22:51:24 d2.utils.events]: \u001b[0meta: 0:06:26  iter: 139  total_loss: 1.300  loss_cls: 0.187  loss_box_reg: 0.247  loss_amodal_mask: 0.311  loss_visible_mask: 0.176  loss_invisible_mask: 0.291  loss_rpn_cls: 0.017  loss_rpn_loc: 0.012  time: 0.2874  data_time: 0.0034  lr: 0.000070  max_mem: 3545M\n",
      "\u001b[32m[02/28 22:51:30 d2.utils.events]: \u001b[0meta: 0:06:23  iter: 159  total_loss: 0.971  loss_cls: 0.146  loss_box_reg: 0.171  loss_amodal_mask: 0.222  loss_visible_mask: 0.177  loss_invisible_mask: 0.182  loss_rpn_cls: 0.020  loss_rpn_loc: 0.011  time: 0.2869  data_time: 0.0035  lr: 0.000080  max_mem: 3545M\n",
      "\u001b[32m[02/28 22:51:35 d2.utils.events]: \u001b[0meta: 0:06:18  iter: 179  total_loss: 1.098  loss_cls: 0.184  loss_box_reg: 0.188  loss_amodal_mask: 0.261  loss_visible_mask: 0.210  loss_invisible_mask: 0.221  loss_rpn_cls: 0.014  loss_rpn_loc: 0.009  time: 0.2873  data_time: 0.0035  lr: 0.000090  max_mem: 3545M\n",
      "\u001b[32m[02/28 22:51:41 d2.utils.events]: \u001b[0meta: 0:06:13  iter: 199  total_loss: 0.893  loss_cls: 0.157  loss_box_reg: 0.147  loss_amodal_mask: 0.188  loss_visible_mask: 0.177  loss_invisible_mask: 0.152  loss_rpn_cls: 0.013  loss_rpn_loc: 0.011  time: 0.2873  data_time: 0.0035  lr: 0.000100  max_mem: 3545M\n",
      "\u001b[32m[02/28 22:51:47 d2.utils.events]: \u001b[0meta: 0:06:07  iter: 219  total_loss: 0.923  loss_cls: 0.157  loss_box_reg: 0.190  loss_amodal_mask: 0.234  loss_visible_mask: 0.173  loss_invisible_mask: 0.170  loss_rpn_cls: 0.011  loss_rpn_loc: 0.010  time: 0.2873  data_time: 0.0035  lr: 0.000110  max_mem: 3545M\n",
      "\u001b[32m[02/28 22:51:53 d2.utils.events]: \u001b[0meta: 0:06:02  iter: 239  total_loss: 1.068  loss_cls: 0.184  loss_box_reg: 0.155  loss_amodal_mask: 0.237  loss_visible_mask: 0.198  loss_invisible_mask: 0.185  loss_rpn_cls: 0.016  loss_rpn_loc: 0.013  time: 0.2874  data_time: 0.0037  lr: 0.000120  max_mem: 3545M\n",
      "\u001b[32m[02/28 22:51:59 d2.utils.events]: \u001b[0meta: 0:05:58  iter: 259  total_loss: 0.847  loss_cls: 0.140  loss_box_reg: 0.162  loss_amodal_mask: 0.171  loss_visible_mask: 0.144  loss_invisible_mask: 0.119  loss_rpn_cls: 0.015  loss_rpn_loc: 0.010  time: 0.2887  data_time: 0.0035  lr: 0.000130  max_mem: 3545M\n",
      "\u001b[32m[02/28 22:52:05 d2.utils.events]: \u001b[0meta: 0:05:52  iter: 279  total_loss: 0.809  loss_cls: 0.124  loss_box_reg: 0.132  loss_amodal_mask: 0.187  loss_visible_mask: 0.163  loss_invisible_mask: 0.152  loss_rpn_cls: 0.009  loss_rpn_loc: 0.008  time: 0.2886  data_time: 0.0035  lr: 0.000140  max_mem: 3545M\n",
      "\u001b[32m[02/28 22:52:10 d2.utils.events]: \u001b[0meta: 0:05:47  iter: 299  total_loss: 0.959  loss_cls: 0.170  loss_box_reg: 0.202  loss_amodal_mask: 0.212  loss_visible_mask: 0.163  loss_invisible_mask: 0.138  loss_rpn_cls: 0.010  loss_rpn_loc: 0.008  time: 0.2887  data_time: 0.0037  lr: 0.000150  max_mem: 3568M\n",
      "\u001b[32m[02/28 22:52:16 d2.utils.events]: \u001b[0meta: 0:05:41  iter: 319  total_loss: 0.775  loss_cls: 0.142  loss_box_reg: 0.144  loss_amodal_mask: 0.209  loss_visible_mask: 0.156  loss_invisible_mask: 0.131  loss_rpn_cls: 0.010  loss_rpn_loc: 0.007  time: 0.2894  data_time: 0.0034  lr: 0.000160  max_mem: 3568M\n",
      "\u001b[32m[02/28 22:52:22 d2.utils.events]: \u001b[0meta: 0:05:36  iter: 339  total_loss: 0.924  loss_cls: 0.131  loss_box_reg: 0.186  loss_amodal_mask: 0.195  loss_visible_mask: 0.166  loss_invisible_mask: 0.128  loss_rpn_cls: 0.013  loss_rpn_loc: 0.014  time: 0.2900  data_time: 0.0037  lr: 0.000170  max_mem: 3568M\n",
      "\u001b[32m[02/28 22:52:28 d2.utils.events]: \u001b[0meta: 0:05:30  iter: 359  total_loss: 0.895  loss_cls: 0.119  loss_box_reg: 0.168  loss_amodal_mask: 0.198  loss_visible_mask: 0.169  loss_invisible_mask: 0.133  loss_rpn_cls: 0.010  loss_rpn_loc: 0.010  time: 0.2897  data_time: 0.0035  lr: 0.000180  max_mem: 3568M\n",
      "\u001b[32m[02/28 22:52:34 d2.utils.events]: \u001b[0meta: 0:05:25  iter: 379  total_loss: 0.963  loss_cls: 0.157  loss_box_reg: 0.183  loss_amodal_mask: 0.200  loss_visible_mask: 0.157  loss_invisible_mask: 0.131  loss_rpn_cls: 0.017  loss_rpn_loc: 0.013  time: 0.2903  data_time: 0.0036  lr: 0.000190  max_mem: 3568M\n",
      "\u001b[32m[02/28 22:52:40 d2.utils.events]: \u001b[0meta: 0:05:19  iter: 399  total_loss: 0.808  loss_cls: 0.127  loss_box_reg: 0.160  loss_amodal_mask: 0.186  loss_visible_mask: 0.166  loss_invisible_mask: 0.114  loss_rpn_cls: 0.008  loss_rpn_loc: 0.009  time: 0.2904  data_time: 0.0036  lr: 0.000200  max_mem: 3568M\n",
      "\u001b[32m[02/28 22:52:46 d2.utils.events]: \u001b[0meta: 0:05:13  iter: 419  total_loss: 0.918  loss_cls: 0.159  loss_box_reg: 0.161  loss_amodal_mask: 0.195  loss_visible_mask: 0.188  loss_invisible_mask: 0.110  loss_rpn_cls: 0.014  loss_rpn_loc: 0.013  time: 0.2906  data_time: 0.0036  lr: 0.000210  max_mem: 3568M\n",
      "\u001b[32m[02/28 22:52:52 d2.utils.events]: \u001b[0meta: 0:05:08  iter: 439  total_loss: 0.877  loss_cls: 0.159  loss_box_reg: 0.179  loss_amodal_mask: 0.225  loss_visible_mask: 0.186  loss_invisible_mask: 0.159  loss_rpn_cls: 0.012  loss_rpn_loc: 0.010  time: 0.2910  data_time: 0.0036  lr: 0.000220  max_mem: 3568M\n",
      "\u001b[32m[02/28 22:52:58 d2.utils.events]: \u001b[0meta: 0:05:03  iter: 459  total_loss: 1.044  loss_cls: 0.125  loss_box_reg: 0.209  loss_amodal_mask: 0.209  loss_visible_mask: 0.166  loss_invisible_mask: 0.147  loss_rpn_cls: 0.014  loss_rpn_loc: 0.010  time: 0.2912  data_time: 0.0035  lr: 0.000230  max_mem: 3721M\n",
      "\u001b[32m[02/28 22:53:04 d2.utils.events]: \u001b[0meta: 0:04:57  iter: 479  total_loss: 1.010  loss_cls: 0.149  loss_box_reg: 0.207  loss_amodal_mask: 0.231  loss_visible_mask: 0.155  loss_invisible_mask: 0.159  loss_rpn_cls: 0.007  loss_rpn_loc: 0.011  time: 0.2915  data_time: 0.0036  lr: 0.000240  max_mem: 3721M\n",
      "\u001b[32m[02/28 22:53:10 d2.utils.events]: \u001b[0meta: 0:04:51  iter: 499  total_loss: 0.904  loss_cls: 0.145  loss_box_reg: 0.151  loss_amodal_mask: 0.223  loss_visible_mask: 0.172  loss_invisible_mask: 0.139  loss_rpn_cls: 0.011  loss_rpn_loc: 0.013  time: 0.2914  data_time: 0.0039  lr: 0.000250  max_mem: 3721M\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[02/28 22:53:16 d2.utils.events]: \u001b[0meta: 0:04:45  iter: 519  total_loss: 1.104  loss_cls: 0.195  loss_box_reg: 0.195  loss_amodal_mask: 0.201  loss_visible_mask: 0.219  loss_invisible_mask: 0.166  loss_rpn_cls: 0.016  loss_rpn_loc: 0.012  time: 0.2916  data_time: 0.0036  lr: 0.000260  max_mem: 3721M\n",
      "\u001b[32m[02/28 22:53:22 d2.utils.events]: \u001b[0meta: 0:04:40  iter: 539  total_loss: 0.899  loss_cls: 0.179  loss_box_reg: 0.197  loss_amodal_mask: 0.176  loss_visible_mask: 0.130  loss_invisible_mask: 0.170  loss_rpn_cls: 0.006  loss_rpn_loc: 0.010  time: 0.2919  data_time: 0.0036  lr: 0.000270  max_mem: 3721M\n",
      "\u001b[32m[02/28 22:53:28 d2.utils.events]: \u001b[0meta: 0:04:34  iter: 559  total_loss: 0.866  loss_cls: 0.145  loss_box_reg: 0.201  loss_amodal_mask: 0.181  loss_visible_mask: 0.168  loss_invisible_mask: 0.110  loss_rpn_cls: 0.012  loss_rpn_loc: 0.011  time: 0.2920  data_time: 0.0040  lr: 0.000280  max_mem: 3721M\n",
      "\u001b[32m[02/28 22:53:34 d2.utils.events]: \u001b[0meta: 0:04:29  iter: 579  total_loss: 0.858  loss_cls: 0.173  loss_box_reg: 0.147  loss_amodal_mask: 0.182  loss_visible_mask: 0.160  loss_invisible_mask: 0.114  loss_rpn_cls: 0.008  loss_rpn_loc: 0.012  time: 0.2922  data_time: 0.0036  lr: 0.000290  max_mem: 3721M\n",
      "\u001b[32m[02/28 22:53:40 d2.utils.events]: \u001b[0meta: 0:04:23  iter: 599  total_loss: 0.790  loss_cls: 0.167  loss_box_reg: 0.177  loss_amodal_mask: 0.179  loss_visible_mask: 0.151  loss_invisible_mask: 0.108  loss_rpn_cls: 0.012  loss_rpn_loc: 0.013  time: 0.2926  data_time: 0.0040  lr: 0.000300  max_mem: 3721M\n",
      "\u001b[32m[02/28 22:53:45 d2.utils.events]: \u001b[0meta: 0:04:17  iter: 619  total_loss: 0.885  loss_cls: 0.152  loss_box_reg: 0.177  loss_amodal_mask: 0.190  loss_visible_mask: 0.162  loss_invisible_mask: 0.123  loss_rpn_cls: 0.011  loss_rpn_loc: 0.014  time: 0.2925  data_time: 0.0036  lr: 0.000310  max_mem: 3721M\n",
      "\u001b[32m[02/28 22:53:51 d2.utils.events]: \u001b[0meta: 0:04:11  iter: 639  total_loss: 0.978  loss_cls: 0.156  loss_box_reg: 0.179  loss_amodal_mask: 0.229  loss_visible_mask: 0.163  loss_invisible_mask: 0.164  loss_rpn_cls: 0.010  loss_rpn_loc: 0.011  time: 0.2926  data_time: 0.0039  lr: 0.000320  max_mem: 3721M\n",
      "\u001b[32m[02/28 22:53:57 d2.utils.events]: \u001b[0meta: 0:04:05  iter: 659  total_loss: 0.797  loss_cls: 0.140  loss_box_reg: 0.157  loss_amodal_mask: 0.186  loss_visible_mask: 0.168  loss_invisible_mask: 0.118  loss_rpn_cls: 0.009  loss_rpn_loc: 0.009  time: 0.2928  data_time: 0.0036  lr: 0.000330  max_mem: 3721M\n",
      "\u001b[32m[02/28 22:54:04 d2.utils.events]: \u001b[0meta: 0:04:00  iter: 679  total_loss: 0.744  loss_cls: 0.104  loss_box_reg: 0.189  loss_amodal_mask: 0.182  loss_visible_mask: 0.171  loss_invisible_mask: 0.056  loss_rpn_cls: 0.005  loss_rpn_loc: 0.012  time: 0.2933  data_time: 0.0037  lr: 0.000340  max_mem: 3721M\n",
      "\u001b[32m[02/28 22:54:10 d2.utils.events]: \u001b[0meta: 0:03:54  iter: 699  total_loss: 0.908  loss_cls: 0.168  loss_box_reg: 0.185  loss_amodal_mask: 0.193  loss_visible_mask: 0.191  loss_invisible_mask: 0.131  loss_rpn_cls: 0.008  loss_rpn_loc: 0.012  time: 0.2937  data_time: 0.0035  lr: 0.000350  max_mem: 3721M\n",
      "\u001b[32m[02/28 22:54:16 d2.utils.events]: \u001b[0meta: 0:03:49  iter: 719  total_loss: 0.798  loss_cls: 0.125  loss_box_reg: 0.175  loss_amodal_mask: 0.176  loss_visible_mask: 0.144  loss_invisible_mask: 0.135  loss_rpn_cls: 0.005  loss_rpn_loc: 0.009  time: 0.2937  data_time: 0.0037  lr: 0.000360  max_mem: 3721M\n",
      "\u001b[32m[02/28 22:54:22 d2.utils.events]: \u001b[0meta: 0:03:43  iter: 739  total_loss: 0.842  loss_cls: 0.113  loss_box_reg: 0.193  loss_amodal_mask: 0.204  loss_visible_mask: 0.166  loss_invisible_mask: 0.133  loss_rpn_cls: 0.005  loss_rpn_loc: 0.009  time: 0.2940  data_time: 0.0036  lr: 0.000370  max_mem: 3721M\n",
      "\u001b[32m[02/28 22:54:28 d2.utils.events]: \u001b[0meta: 0:03:37  iter: 759  total_loss: 1.053  loss_cls: 0.171  loss_box_reg: 0.200  loss_amodal_mask: 0.236  loss_visible_mask: 0.196  loss_invisible_mask: 0.176  loss_rpn_cls: 0.007  loss_rpn_loc: 0.013  time: 0.2942  data_time: 0.0035  lr: 0.000380  max_mem: 3721M\n",
      "\u001b[32m[02/28 22:54:34 d2.utils.events]: \u001b[0meta: 0:03:32  iter: 779  total_loss: 0.882  loss_cls: 0.158  loss_box_reg: 0.185  loss_amodal_mask: 0.186  loss_visible_mask: 0.169  loss_invisible_mask: 0.113  loss_rpn_cls: 0.009  loss_rpn_loc: 0.016  time: 0.2945  data_time: 0.0036  lr: 0.000390  max_mem: 3721M\n",
      "\u001b[32m[02/28 22:54:40 d2.utils.events]: \u001b[0meta: 0:03:26  iter: 799  total_loss: 0.926  loss_cls: 0.183  loss_box_reg: 0.215  loss_amodal_mask: 0.193  loss_visible_mask: 0.159  loss_invisible_mask: 0.114  loss_rpn_cls: 0.020  loss_rpn_loc: 0.012  time: 0.2945  data_time: 0.0037  lr: 0.000400  max_mem: 3721M\n",
      "\u001b[32m[02/28 22:54:46 d2.utils.events]: \u001b[0meta: 0:03:20  iter: 819  total_loss: 0.986  loss_cls: 0.184  loss_box_reg: 0.168  loss_amodal_mask: 0.191  loss_visible_mask: 0.155  loss_invisible_mask: 0.122  loss_rpn_cls: 0.008  loss_rpn_loc: 0.015  time: 0.2947  data_time: 0.0037  lr: 0.000410  max_mem: 3721M\n",
      "\u001b[32m[02/28 22:54:52 d2.utils.events]: \u001b[0meta: 0:03:14  iter: 839  total_loss: 0.999  loss_cls: 0.207  loss_box_reg: 0.224  loss_amodal_mask: 0.206  loss_visible_mask: 0.162  loss_invisible_mask: 0.139  loss_rpn_cls: 0.006  loss_rpn_loc: 0.013  time: 0.2950  data_time: 0.0037  lr: 0.000420  max_mem: 3721M\n",
      "\u001b[32m[02/28 22:54:58 d2.utils.events]: \u001b[0meta: 0:03:08  iter: 859  total_loss: 1.062  loss_cls: 0.151  loss_box_reg: 0.236  loss_amodal_mask: 0.228  loss_visible_mask: 0.197  loss_invisible_mask: 0.129  loss_rpn_cls: 0.005  loss_rpn_loc: 0.015  time: 0.2950  data_time: 0.0036  lr: 0.000430  max_mem: 3721M\n",
      "\u001b[32m[02/28 22:55:04 d2.utils.events]: \u001b[0meta: 0:03:03  iter: 879  total_loss: 0.722  loss_cls: 0.154  loss_box_reg: 0.147  loss_amodal_mask: 0.165  loss_visible_mask: 0.128  loss_invisible_mask: 0.127  loss_rpn_cls: 0.006  loss_rpn_loc: 0.009  time: 0.2951  data_time: 0.0038  lr: 0.000440  max_mem: 3721M\n",
      "\u001b[32m[02/28 22:55:10 d2.utils.events]: \u001b[0meta: 0:02:57  iter: 899  total_loss: 1.045  loss_cls: 0.192  loss_box_reg: 0.224  loss_amodal_mask: 0.242  loss_visible_mask: 0.184  loss_invisible_mask: 0.162  loss_rpn_cls: 0.009  loss_rpn_loc: 0.016  time: 0.2953  data_time: 0.0037  lr: 0.000450  max_mem: 3721M\n",
      "\u001b[32m[02/28 22:55:16 d2.utils.events]: \u001b[0meta: 0:02:51  iter: 919  total_loss: 0.922  loss_cls: 0.163  loss_box_reg: 0.186  loss_amodal_mask: 0.220  loss_visible_mask: 0.168  loss_invisible_mask: 0.152  loss_rpn_cls: 0.009  loss_rpn_loc: 0.011  time: 0.2952  data_time: 0.0037  lr: 0.000460  max_mem: 3721M\n",
      "\u001b[32m[02/28 22:55:22 d2.utils.events]: \u001b[0meta: 0:02:45  iter: 939  total_loss: 0.814  loss_cls: 0.116  loss_box_reg: 0.145  loss_amodal_mask: 0.182  loss_visible_mask: 0.173  loss_invisible_mask: 0.094  loss_rpn_cls: 0.006  loss_rpn_loc: 0.008  time: 0.2953  data_time: 0.0040  lr: 0.000470  max_mem: 3721M\n",
      "\u001b[32m[02/28 22:55:28 d2.utils.events]: \u001b[0meta: 0:02:39  iter: 959  total_loss: 0.963  loss_cls: 0.144  loss_box_reg: 0.185  loss_amodal_mask: 0.207  loss_visible_mask: 0.216  loss_invisible_mask: 0.166  loss_rpn_cls: 0.004  loss_rpn_loc: 0.018  time: 0.2954  data_time: 0.0038  lr: 0.000480  max_mem: 3721M\n",
      "\u001b[32m[02/28 22:55:34 d2.utils.events]: \u001b[0meta: 0:02:33  iter: 979  total_loss: 0.966  loss_cls: 0.147  loss_box_reg: 0.175  loss_amodal_mask: 0.223  loss_visible_mask: 0.166  loss_invisible_mask: 0.133  loss_rpn_cls: 0.008  loss_rpn_loc: 0.010  time: 0.2953  data_time: 0.0037  lr: 0.000490  max_mem: 3721M\n",
      "\u001b[32m[02/28 22:55:40 d2.utils.events]: \u001b[0meta: 0:02:27  iter: 999  total_loss: 0.926  loss_cls: 0.228  loss_box_reg: 0.205  loss_amodal_mask: 0.201  loss_visible_mask: 0.167  loss_invisible_mask: 0.123  loss_rpn_cls: 0.006  loss_rpn_loc: 0.015  time: 0.2955  data_time: 0.0039  lr: 0.000500  max_mem: 3721M\n",
      "\u001b[32m[02/28 22:55:46 d2.utils.events]: \u001b[0meta: 0:02:22  iter: 1019  total_loss: 0.983  loss_cls: 0.186  loss_box_reg: 0.192  loss_amodal_mask: 0.212  loss_visible_mask: 0.185  loss_invisible_mask: 0.112  loss_rpn_cls: 0.010  loss_rpn_loc: 0.010  time: 0.2953  data_time: 0.0036  lr: 0.000500  max_mem: 3721M\n",
      "\u001b[32m[02/28 22:55:53 d2.utils.events]: \u001b[0meta: 0:02:16  iter: 1039  total_loss: 0.832  loss_cls: 0.158  loss_box_reg: 0.179  loss_amodal_mask: 0.187  loss_visible_mask: 0.170  loss_invisible_mask: 0.164  loss_rpn_cls: 0.005  loss_rpn_loc: 0.013  time: 0.2956  data_time: 0.0165  lr: 0.000500  max_mem: 3721M\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[02/28 22:55:59 d2.utils.events]: \u001b[0meta: 0:02:10  iter: 1059  total_loss: 0.869  loss_cls: 0.132  loss_box_reg: 0.164  loss_amodal_mask: 0.198  loss_visible_mask: 0.204  loss_invisible_mask: 0.116  loss_rpn_cls: 0.006  loss_rpn_loc: 0.011  time: 0.2958  data_time: 0.0036  lr: 0.000500  max_mem: 3721M\n",
      "\u001b[32m[02/28 22:56:04 d2.utils.events]: \u001b[0meta: 0:02:04  iter: 1079  total_loss: 0.909  loss_cls: 0.164  loss_box_reg: 0.195  loss_amodal_mask: 0.211  loss_visible_mask: 0.183  loss_invisible_mask: 0.131  loss_rpn_cls: 0.008  loss_rpn_loc: 0.010  time: 0.2955  data_time: 0.0038  lr: 0.000500  max_mem: 3721M\n",
      "\u001b[32m[02/28 22:56:11 d2.utils.events]: \u001b[0meta: 0:01:59  iter: 1099  total_loss: 0.924  loss_cls: 0.152  loss_box_reg: 0.202  loss_amodal_mask: 0.189  loss_visible_mask: 0.155  loss_invisible_mask: 0.116  loss_rpn_cls: 0.008  loss_rpn_loc: 0.013  time: 0.2957  data_time: 0.0036  lr: 0.000500  max_mem: 3721M\n",
      "\u001b[32m[02/28 22:56:17 d2.utils.events]: \u001b[0meta: 0:01:53  iter: 1119  total_loss: 0.830  loss_cls: 0.140  loss_box_reg: 0.188  loss_amodal_mask: 0.180  loss_visible_mask: 0.165  loss_invisible_mask: 0.101  loss_rpn_cls: 0.004  loss_rpn_loc: 0.012  time: 0.2959  data_time: 0.0035  lr: 0.000500  max_mem: 3721M\n",
      "\u001b[32m[02/28 22:56:23 d2.utils.events]: \u001b[0meta: 0:01:47  iter: 1139  total_loss: 0.967  loss_cls: 0.192  loss_box_reg: 0.182  loss_amodal_mask: 0.204  loss_visible_mask: 0.181  loss_invisible_mask: 0.109  loss_rpn_cls: 0.009  loss_rpn_loc: 0.012  time: 0.2959  data_time: 0.0037  lr: 0.000500  max_mem: 3721M\n",
      "\u001b[32m[02/28 22:56:29 d2.utils.events]: \u001b[0meta: 0:01:41  iter: 1159  total_loss: 0.872  loss_cls: 0.177  loss_box_reg: 0.166  loss_amodal_mask: 0.191  loss_visible_mask: 0.128  loss_invisible_mask: 0.145  loss_rpn_cls: 0.007  loss_rpn_loc: 0.014  time: 0.2959  data_time: 0.0036  lr: 0.000500  max_mem: 3721M\n",
      "\u001b[32m[02/28 22:56:35 d2.utils.events]: \u001b[0meta: 0:01:35  iter: 1179  total_loss: 0.679  loss_cls: 0.111  loss_box_reg: 0.126  loss_amodal_mask: 0.157  loss_visible_mask: 0.152  loss_invisible_mask: 0.089  loss_rpn_cls: 0.005  loss_rpn_loc: 0.010  time: 0.2960  data_time: 0.0037  lr: 0.000500  max_mem: 3721M\n",
      "\u001b[32m[02/28 22:56:41 d2.utils.events]: \u001b[0meta: 0:01:29  iter: 1199  total_loss: 0.768  loss_cls: 0.098  loss_box_reg: 0.150  loss_amodal_mask: 0.175  loss_visible_mask: 0.178  loss_invisible_mask: 0.091  loss_rpn_cls: 0.005  loss_rpn_loc: 0.012  time: 0.2960  data_time: 0.0034  lr: 0.000500  max_mem: 3721M\n",
      "\u001b[32m[02/28 22:56:47 d2.utils.events]: \u001b[0meta: 0:01:23  iter: 1219  total_loss: 0.671  loss_cls: 0.116  loss_box_reg: 0.157  loss_amodal_mask: 0.163  loss_visible_mask: 0.167  loss_invisible_mask: 0.083  loss_rpn_cls: 0.003  loss_rpn_loc: 0.010  time: 0.2960  data_time: 0.0034  lr: 0.000500  max_mem: 3721M\n",
      "\u001b[32m[02/28 22:56:53 d2.utils.events]: \u001b[0meta: 0:01:17  iter: 1239  total_loss: 0.749  loss_cls: 0.123  loss_box_reg: 0.169  loss_amodal_mask: 0.175  loss_visible_mask: 0.187  loss_invisible_mask: 0.098  loss_rpn_cls: 0.006  loss_rpn_loc: 0.008  time: 0.2961  data_time: 0.0038  lr: 0.000500  max_mem: 3721M\n",
      "\u001b[32m[02/28 22:56:59 d2.utils.events]: \u001b[0meta: 0:01:11  iter: 1259  total_loss: 1.024  loss_cls: 0.164  loss_box_reg: 0.247  loss_amodal_mask: 0.201  loss_visible_mask: 0.183  loss_invisible_mask: 0.114  loss_rpn_cls: 0.010  loss_rpn_loc: 0.015  time: 0.2962  data_time: 0.0037  lr: 0.000500  max_mem: 3721M\n",
      "\u001b[32m[02/28 22:57:05 d2.utils.events]: \u001b[0meta: 0:01:06  iter: 1279  total_loss: 0.863  loss_cls: 0.192  loss_box_reg: 0.188  loss_amodal_mask: 0.179  loss_visible_mask: 0.167  loss_invisible_mask: 0.158  loss_rpn_cls: 0.005  loss_rpn_loc: 0.010  time: 0.2964  data_time: 0.0034  lr: 0.000500  max_mem: 3721M\n",
      "\u001b[32m[02/28 22:57:11 d2.utils.events]: \u001b[0meta: 0:01:00  iter: 1299  total_loss: 1.063  loss_cls: 0.179  loss_box_reg: 0.257  loss_amodal_mask: 0.229  loss_visible_mask: 0.202  loss_invisible_mask: 0.143  loss_rpn_cls: 0.009  loss_rpn_loc: 0.013  time: 0.2965  data_time: 0.0036  lr: 0.000500  max_mem: 3721M\n",
      "\u001b[32m[02/28 22:57:17 d2.utils.events]: \u001b[0meta: 0:00:54  iter: 1319  total_loss: 0.996  loss_cls: 0.193  loss_box_reg: 0.224  loss_amodal_mask: 0.200  loss_visible_mask: 0.210  loss_invisible_mask: 0.101  loss_rpn_cls: 0.008  loss_rpn_loc: 0.011  time: 0.2966  data_time: 0.0037  lr: 0.000050  max_mem: 3721M\n",
      "\u001b[32m[02/28 22:57:23 d2.utils.events]: \u001b[0meta: 0:00:48  iter: 1339  total_loss: 0.794  loss_cls: 0.133  loss_box_reg: 0.164  loss_amodal_mask: 0.158  loss_visible_mask: 0.173  loss_invisible_mask: 0.083  loss_rpn_cls: 0.007  loss_rpn_loc: 0.011  time: 0.2965  data_time: 0.0035  lr: 0.000050  max_mem: 3721M\n",
      "\u001b[32m[02/28 22:57:29 d2.utils.events]: \u001b[0meta: 0:00:42  iter: 1359  total_loss: 0.984  loss_cls: 0.154  loss_box_reg: 0.217  loss_amodal_mask: 0.216  loss_visible_mask: 0.181  loss_invisible_mask: 0.127  loss_rpn_cls: 0.007  loss_rpn_loc: 0.015  time: 0.2965  data_time: 0.0035  lr: 0.000050  max_mem: 3721M\n",
      "\u001b[32m[02/28 22:57:35 d2.utils.events]: \u001b[0meta: 0:00:36  iter: 1379  total_loss: 0.935  loss_cls: 0.178  loss_box_reg: 0.227  loss_amodal_mask: 0.198  loss_visible_mask: 0.157  loss_invisible_mask: 0.161  loss_rpn_cls: 0.006  loss_rpn_loc: 0.017  time: 0.2968  data_time: 0.0038  lr: 0.000050  max_mem: 3721M\n",
      "\u001b[32m[02/28 22:57:41 d2.utils.events]: \u001b[0meta: 0:00:30  iter: 1399  total_loss: 0.943  loss_cls: 0.194  loss_box_reg: 0.225  loss_amodal_mask: 0.205  loss_visible_mask: 0.167  loss_invisible_mask: 0.141  loss_rpn_cls: 0.008  loss_rpn_loc: 0.013  time: 0.2970  data_time: 0.0038  lr: 0.000050  max_mem: 3721M\n",
      "\u001b[32m[02/28 22:57:47 d2.utils.events]: \u001b[0meta: 0:00:24  iter: 1419  total_loss: 0.712  loss_cls: 0.162  loss_box_reg: 0.155  loss_amodal_mask: 0.157  loss_visible_mask: 0.137  loss_invisible_mask: 0.109  loss_rpn_cls: 0.006  loss_rpn_loc: 0.010  time: 0.2971  data_time: 0.0036  lr: 0.000005  max_mem: 3721M\n",
      "\u001b[32m[02/28 22:57:54 d2.utils.events]: \u001b[0meta: 0:00:18  iter: 1439  total_loss: 0.832  loss_cls: 0.141  loss_box_reg: 0.181  loss_amodal_mask: 0.179  loss_visible_mask: 0.145  loss_invisible_mask: 0.117  loss_rpn_cls: 0.005  loss_rpn_loc: 0.012  time: 0.2973  data_time: 0.0035  lr: 0.000005  max_mem: 3721M\n",
      "\u001b[32m[02/28 22:58:00 d2.utils.events]: \u001b[0meta: 0:00:12  iter: 1459  total_loss: 0.848  loss_cls: 0.162  loss_box_reg: 0.184  loss_amodal_mask: 0.203  loss_visible_mask: 0.159  loss_invisible_mask: 0.111  loss_rpn_cls: 0.007  loss_rpn_loc: 0.012  time: 0.2972  data_time: 0.0034  lr: 0.000005  max_mem: 3721M\n",
      "\u001b[32m[02/28 22:58:05 d2.utils.events]: \u001b[0meta: 0:00:06  iter: 1479  total_loss: 0.664  loss_cls: 0.110  loss_box_reg: 0.177  loss_amodal_mask: 0.154  loss_visible_mask: 0.137  loss_invisible_mask: 0.086  loss_rpn_cls: 0.005  loss_rpn_loc: 0.007  time: 0.2972  data_time: 0.0036  lr: 0.000005  max_mem: 3721M\n",
      "\u001b[32m[02/28 22:58:12 d2.data.datasets.coco]: \u001b[0mLoaded 1223 images in COCO format from datasets/coco/annotations/COCO_amodal_val2014_with_classes_poly.json\n",
      "\u001b[32m[02/28 22:58:12 d2.data.build]: \u001b[0mDistribution of instances among all 80 categories:\n",
      "\u001b[36m|   category    | #instances   |   category   | #instances   |   category    | #instances   |\n",
      "|:-------------:|:-------------|:------------:|:-------------|:-------------:|:-------------|\n",
      "|    person     | 1197         |   bicycle    | 12           |      car      | 154          |\n",
      "|  motorcycle   | 31           |   airplane   | 39           |      bus      | 44           |\n",
      "|     train     | 42           |    truck     | 44           |     boat      | 49           |\n",
      "| traffic light | 7            | fire hydrant | 17           |   stop sign   | 9            |\n",
      "| parking meter | 2            |    bench     | 34           |     bird      | 42           |\n",
      "|      cat      | 42           |     dog      | 62           |     horse     | 48           |\n",
      "|     sheep     | 49           |     cow      | 56           |   elephant    | 43           |\n",
      "|     bear      | 11           |    zebra     | 41           |    giraffe    | 31           |\n",
      "|   backpack    | 12           |   umbrella   | 73           |    handbag    | 24           |\n",
      "|      tie      | 23           |   suitcase   | 42           |    frisbee    | 27           |\n",
      "|     skis      | 9            |  snowboard   | 9            |  sports ball  | 14           |\n",
      "|     kite      | 32           | baseball bat | 18           | baseball gl.. | 12           |\n",
      "|  skateboard   | 16           |  surfboard   | 14           | tennis racket | 38           |\n",
      "|    bottle     | 91           |  wine glass  | 26           |      cup      | 141          |\n",
      "|     fork      | 19           |    knife     | 26           |     spoon     | 22           |\n",
      "|     bowl      | 51           |    banana    | 20           |     apple     | 19           |\n",
      "|   sandwich    | 28           |    orange    | 17           |   broccoli    | 16           |\n",
      "|    carrot     | 10           |   hot dog    | 23           |     pizza     | 60           |\n",
      "|     donut     | 54           |     cake     | 33           |     chair     | 127          |\n",
      "|     couch     | 26           | potted plant | 15           |      bed      | 12           |\n",
      "| dining table  | 32           |    toilet    | 45           |      tv       | 53           |\n",
      "|    laptop     | 52           |    mouse     | 23           |    remote     | 18           |\n",
      "|   keyboard    | 28           |  cell phone  | 31           |   microwave   | 8            |\n",
      "|     oven      | 11           |   toaster    | 2            |     sink      | 24           |\n",
      "| refrigerator  | 13           |     book     | 23           |     clock     | 33           |\n",
      "|     vase      | 42           |   scissors   | 12           |  teddy bear   | 41           |\n",
      "|  hair drier   | 1            |  toothbrush  | 2            |               |              |\n",
      "|     total     | 3799         |              |              |               |              |\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[02/28 22:58:12 d2.engine.defaults]: \u001b[0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n",
      "\u001b[32m[02/28 22:58:12 d2.utils.events]: \u001b[0meta: 0:00:00  iter: 1499  total_loss: 0.982  loss_cls: 0.133  loss_box_reg: 0.242  loss_amodal_mask: 0.198  loss_visible_mask: 0.206  loss_invisible_mask: 0.134  loss_rpn_cls: 0.003  loss_rpn_loc: 0.010  time: 0.2973  data_time: 0.0036  lr: 0.000005  max_mem: 3721M\n",
      "\u001b[32m[02/28 22:58:12 d2.engine.hooks]: \u001b[0mOverall training speed: 1497 iterations in 0:07:25 (0.2975 s / it)\n",
      "\u001b[32m[02/28 22:58:12 d2.engine.hooks]: \u001b[0mTotal training time: 0:07:28 (0:00:03 on hooks)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from detectron2.engine import DefaultTrainer\n",
    "from detectron2.config import get_cfg\n",
    "\n",
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_orcnn_R_50_FPN_3x.yaml\"))\n",
    "cfg.DATASETS.TRAIN = (\"amodal_coco_train\",)\n",
    "cfg.DATASETS.TEST = (\"amodal_coco_val\",)\n",
    "cfg.DATALOADER.NUM_WORKERS = 2\n",
    "cfg.SOLVER.IMS_PER_BATCH = 2\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 80\n",
    "cfg.SOLVER.BASE_LR = 0.0005  # pick a good LR\n",
    "cfg.SOLVER.STEPS = (1300,1400)\n",
    "cfg.SOLVER.MAX_ITER = 1500 \n",
    "cfg.VIS_PERIOD = 1000\n",
    "cfg.OUTPUT_DIR = \"myAmodalCheckpoint\"\n",
    "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.9  # set the testing threshold for this model\n",
    "trainer = DefaultTrainer(cfg) \n",
    "trainer.resume_or_load(resume=False)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow installation not found - running with reduced feature set.\n",
      "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n",
      "TensorBoard 2.1.0 at http://localhost:6006/ (Press CTRL+C to quit)\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!tensorboard --logdir=myAmodalCheckpoint --port=6006"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evalution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[02/28 22:59:41 d2.data.datasets.coco]: \u001b[0mLoaded 1223 images in COCO format from datasets/coco/annotations/COCO_amodal_val2014_with_classes_poly.json\n",
      "\u001b[32m[02/28 22:59:41 d2.evaluation.evaluator]: \u001b[0mStart inference on 1223 images\n",
      "\u001b[32m[02/28 22:59:42 d2.evaluation.evaluator]: \u001b[0mInference done 11/1223. 0.0542 s / img. ETA=0:01:11\n",
      "\u001b[32m[02/28 22:59:47 d2.evaluation.evaluator]: \u001b[0mInference done 97/1223. 0.0546 s / img. ETA=0:01:05\n",
      "\u001b[32m[02/28 22:59:52 d2.evaluation.evaluator]: \u001b[0mInference done 184/1223. 0.0545 s / img. ETA=0:01:00\n",
      "\u001b[32m[02/28 22:59:57 d2.evaluation.evaluator]: \u001b[0mInference done 269/1223. 0.0547 s / img. ETA=0:00:55\n",
      "\u001b[32m[02/28 23:00:02 d2.evaluation.evaluator]: \u001b[0mInference done 353/1223. 0.0550 s / img. ETA=0:00:51\n",
      "\u001b[32m[02/28 23:00:07 d2.evaluation.evaluator]: \u001b[0mInference done 439/1223. 0.0550 s / img. ETA=0:00:46\n",
      "\u001b[32m[02/28 23:00:12 d2.evaluation.evaluator]: \u001b[0mInference done 524/1223. 0.0551 s / img. ETA=0:00:41\n",
      "\u001b[32m[02/28 23:00:17 d2.evaluation.evaluator]: \u001b[0mInference done 608/1223. 0.0551 s / img. ETA=0:00:36\n",
      "\u001b[32m[02/28 23:00:22 d2.evaluation.evaluator]: \u001b[0mInference done 693/1223. 0.0552 s / img. ETA=0:00:31\n",
      "\u001b[32m[02/28 23:00:27 d2.evaluation.evaluator]: \u001b[0mInference done 778/1223. 0.0552 s / img. ETA=0:00:26\n",
      "\u001b[32m[02/28 23:00:32 d2.evaluation.evaluator]: \u001b[0mInference done 863/1223. 0.0552 s / img. ETA=0:00:21\n",
      "\u001b[32m[02/28 23:00:37 d2.evaluation.evaluator]: \u001b[0mInference done 948/1223. 0.0552 s / img. ETA=0:00:16\n",
      "\u001b[32m[02/28 23:00:42 d2.evaluation.evaluator]: \u001b[0mInference done 1033/1223. 0.0552 s / img. ETA=0:00:11\n",
      "\u001b[32m[02/28 23:00:47 d2.evaluation.evaluator]: \u001b[0mInference done 1118/1223. 0.0552 s / img. ETA=0:00:06\n",
      "\u001b[32m[02/28 23:00:52 d2.evaluation.evaluator]: \u001b[0mInference done 1202/1223. 0.0553 s / img. ETA=0:00:01\n",
      "\u001b[32m[02/28 23:00:54 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:01:12.026627 (0.059135 s / img per device, on 1 devices)\n",
      "\u001b[32m[02/28 23:00:54 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:01:07 (0.055234 s / img per device, on 1 devices)\n",
      "\u001b[32m[02/28 23:00:54 d2.evaluation.AmodalCocoEvaluation]: \u001b[0mPreparing results for COCO format ...\n",
      "\u001b[32m[02/28 23:00:54 d2.evaluation.AmodalCocoEvaluation]: \u001b[0mSaving results to myAmodalEvaluation/coco_instances_results.json\n",
      "\u001b[32m[02/28 23:00:54 d2.evaluation.AmodalCocoEvaluation]: \u001b[0mEvaluating predictions ...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=2.19s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.77s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.307\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.394\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.350\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.128\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.284\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.387\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.287\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.353\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.353\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.130\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.318\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.443\n",
      "\u001b[32m[02/28 23:00:57 d2.evaluation.AmodalCocoEvaluation]: \u001b[0mEvaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |\n",
      "|:------:|:------:|:------:|:------:|:------:|:------:|\n",
      "| 30.681 | 39.369 | 35.006 | 12.810 | 28.411 | 38.659 |\n",
      "\u001b[32m[02/28 23:00:57 d2.evaluation.AmodalCocoEvaluation]: \u001b[0mPer-category bbox AP: \n",
      "| category      | AP     | category     | AP     | category       | AP     |\n",
      "|:--------------|:-------|:-------------|:-------|:---------------|:-------|\n",
      "| person        | 51.227 | bicycle      | 12.871 | car            | 22.640 |\n",
      "| motorcycle    | 38.151 | airplane     | 64.129 | bus            | 54.868 |\n",
      "| train         | 53.046 | truck        | 26.323 | boat           | 16.155 |\n",
      "| traffic light | 2.376  | fire hydrant | 69.288 | stop sign      | 10.863 |\n",
      "| parking meter | 0.000  | bench        | 41.045 | bird           | 27.792 |\n",
      "| cat           | 60.171 | dog          | 33.595 | horse          | 55.908 |\n",
      "| sheep         | 55.178 | cow          | 38.939 | elephant       | 52.144 |\n",
      "| bear          | 72.279 | zebra        | 61.927 | giraffe        | 75.727 |\n",
      "| backpack      | 0.000  | umbrella     | 22.903 | handbag        | 0.000  |\n",
      "| tie           | 0.000  | suitcase     | 27.015 | frisbee        | 46.324 |\n",
      "| skis          | 0.000  | snowboard    | 23.296 | sports ball    | 38.577 |\n",
      "| kite          | 44.342 | baseball bat | 10.941 | baseball glove | 0.000  |\n",
      "| skateboard    | 41.413 | surfboard    | 30.198 | tennis racket  | 55.806 |\n",
      "| bottle        | 40.474 | wine glass   | 53.682 | cup            | 34.668 |\n",
      "| fork          | 12.720 | knife        | 7.195  | spoon          | 17.762 |\n",
      "| bowl          | 10.901 | banana       | 0.000  | apple          | 29.719 |\n",
      "| sandwich      | 19.248 | orange       | 58.443 | broccoli       | 15.743 |\n",
      "| carrot        | 0.000  | hot dog      | 39.658 | pizza          | 39.560 |\n",
      "| donut         | 33.491 | cake         | 39.682 | chair          | 23.822 |\n",
      "| couch         | 25.874 | potted plant | 0.000  | bed            | 26.891 |\n",
      "| dining table  | 3.564  | toilet       | 60.626 | tv             | 60.000 |\n",
      "| laptop        | 61.031 | mouse        | 28.919 | remote         | 17.739 |\n",
      "| keyboard      | 41.720 | cell phone   | 26.205 | microwave      | 24.323 |\n",
      "| oven          | 29.307 | toaster      | 0.000  | sink           | 29.348 |\n",
      "| refrigerator  | 47.186 | book         | 9.485  | clock          | 43.639 |\n",
      "| vase          | 18.203 | scissors     | 17.030 | teddy bear     | 23.702 |\n",
      "| hair drier    | 0.000  | toothbrush   | 45.446 |                |        |\n",
      "Loading and preparing results...\n",
      "DONE (t=0.06s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *segm*\n",
      "DONE (t=2.47s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.77s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.293\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.402\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.335\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.096\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.263\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.383\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.275\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.336\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.337\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.113\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.303\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.423\n",
      "\u001b[32m[02/28 23:01:00 d2.evaluation.AmodalCocoEvaluation]: \u001b[0mEvaluation results for segm: \n",
      "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
      "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
      "| 29.300 | 40.240 | 33.453 | 9.615 | 26.346 | 38.253 |\n",
      "\u001b[32m[02/28 23:01:00 d2.evaluation.AmodalCocoEvaluation]: \u001b[0mPer-category segm AP: \n",
      "| category      | AP     | category     | AP     | category       | AP     |\n",
      "|:--------------|:-------|:-------------|:-------|:---------------|:-------|\n",
      "| person        | 45.780 | bicycle      | 6.658  | car            | 22.823 |\n",
      "| motorcycle    | 32.711 | airplane     | 47.098 | bus            | 55.888 |\n",
      "| train         | 54.306 | truck        | 26.423 | boat           | 19.628 |\n",
      "| traffic light | 2.376  | fire hydrant | 65.984 | stop sign      | 24.661 |\n",
      "| parking meter | 0.000  | bench        | 36.720 | bird           | 23.803 |\n",
      "| cat           | 68.689 | dog          | 29.803 | horse          | 42.031 |\n",
      "| sheep         | 49.827 | cow          | 33.951 | elephant       | 43.730 |\n",
      "| bear          | 72.370 | zebra        | 48.521 | giraffe        | 49.125 |\n",
      "| backpack      | 0.000  | umbrella     | 29.164 | handbag        | 0.000  |\n",
      "| tie           | 0.000  | suitcase     | 26.498 | frisbee        | 46.147 |\n",
      "| skis          | 0.000  | snowboard    | 21.143 | sports ball    | 38.317 |\n",
      "| kite          | 39.674 | baseball bat | 9.710  | baseball glove | 0.000  |\n",
      "| skateboard    | 32.316 | surfboard    | 35.276 | tennis racket  | 57.408 |\n",
      "| bottle        | 37.412 | wine glass   | 48.177 | cup            | 36.532 |\n",
      "| fork          | 7.957  | knife        | 6.374  | spoon          | 17.822 |\n",
      "| bowl          | 11.084 | banana       | 0.000  | apple          | 31.733 |\n",
      "| sandwich      | 19.921 | orange       | 57.112 | broccoli       | 15.149 |\n",
      "| carrot        | 0.000  | hot dog      | 39.434 | pizza          | 40.016 |\n",
      "| donut         | 33.821 | cake         | 41.079 | chair          | 18.470 |\n",
      "| couch         | 26.507 | potted plant | 0.000  | bed            | 26.699 |\n",
      "| dining table  | 1.584  | toilet       | 57.879 | tv             | 64.013 |\n",
      "| laptop        | 59.913 | mouse        | 45.048 | remote         | 18.787 |\n",
      "| keyboard      | 47.458 | cell phone   | 26.825 | microwave      | 22.871 |\n",
      "| oven          | 25.380 | toaster      | 0.000  | sink           | 32.215 |\n",
      "| refrigerator  | 43.983 | book         | 7.228  | clock          | 43.724 |\n",
      "| vase          | 19.102 | scissors     | 12.317 | teddy bear     | 21.444 |\n",
      "| hair drier    | 0.000  | toothbrush   | 40.396 |                |        |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preparing results...\n",
      "DONE (t=0.06s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *visible*\n",
      "DONE (t=2.34s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.77s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.312\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.405\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.360\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.103\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.277\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.409\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.287\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.354\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.354\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.118\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.317\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.449\n",
      "\u001b[32m[02/28 23:01:03 d2.evaluation.AmodalCocoEvaluation]: \u001b[0mEvaluation results for visible: \n",
      "|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |\n",
      "|:------:|:------:|:------:|:------:|:------:|:------:|\n",
      "| 31.205 | 40.543 | 35.950 | 10.304 | 27.730 | 40.874 |\n",
      "\u001b[32m[02/28 23:01:03 d2.evaluation.AmodalCocoEvaluation]: \u001b[0mPer-category visible AP: \n",
      "| category      | AP     | category     | AP     | category       | AP     |\n",
      "|:--------------|:-------|:-------------|:-------|:---------------|:-------|\n",
      "| person        | 48.435 | bicycle      | 9.109  | car            | 24.021 |\n",
      "| motorcycle    | 31.109 | airplane     | 52.442 | bus            | 58.602 |\n",
      "| train         | 57.100 | truck        | 26.149 | boat           | 18.454 |\n",
      "| traffic light | 2.673  | fire hydrant | 71.443 | stop sign      | 27.214 |\n",
      "| parking meter | 0.000  | bench        | 32.164 | bird           | 24.693 |\n",
      "| cat           | 68.241 | dog          | 31.344 | horse          | 44.483 |\n",
      "| sheep         | 55.764 | cow          | 34.817 | elephant       | 50.153 |\n",
      "| bear          | 82.175 | zebra        | 54.643 | giraffe        | 60.523 |\n",
      "| backpack      | 0.000  | umbrella     | 31.223 | handbag        | 0.000  |\n",
      "| tie           | 0.000  | suitcase     | 27.507 | frisbee        | 44.628 |\n",
      "| skis          | 0.000  | snowboard    | 17.553 | sports ball    | 38.594 |\n",
      "| kite          | 41.706 | baseball bat | 13.762 | baseball glove | 0.000  |\n",
      "| skateboard    | 36.266 | surfboard    | 38.956 | tennis racket  | 59.411 |\n",
      "| bottle        | 40.485 | wine glass   | 52.912 | cup            | 37.343 |\n",
      "| fork          | 8.042  | knife        | 8.043  | spoon          | 18.094 |\n",
      "| bowl          | 11.393 | banana       | 0.000  | apple          | 32.393 |\n",
      "| sandwich      | 20.442 | orange       | 59.072 | broccoli       | 15.149 |\n",
      "| carrot        | 0.000  | hot dog      | 41.368 | pizza          | 40.818 |\n",
      "| donut         | 36.681 | cake         | 43.021 | chair          | 19.576 |\n",
      "| couch         | 22.529 | potted plant | 0.000  | bed            | 27.209 |\n",
      "| dining table  | 1.980  | toilet       | 62.384 | tv             | 64.430 |\n",
      "| laptop        | 62.168 | mouse        | 56.135 | remote         | 19.208 |\n",
      "| keyboard      | 50.299 | cell phone   | 31.300 | microwave      | 38.053 |\n",
      "| oven          | 25.297 | toaster      | 0.000  | sink           | 33.021 |\n",
      "| refrigerator  | 51.670 | book         | 10.376 | clock          | 45.292 |\n",
      "| vase          | 20.823 | scissors     | 11.525 | teddy bear     | 22.115 |\n",
      "| hair drier    | 0.000  | toothbrush   | 40.396 |                |        |\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OrderedDict([('bbox',\n",
       "              {'AP': 30.68075979372943,\n",
       "               'AP50': 39.36921086052871,\n",
       "               'AP75': 35.005607718478956,\n",
       "               'APs': 12.810229683560658,\n",
       "               'APm': 28.411168559161876,\n",
       "               'APl': 38.65940296878257,\n",
       "               'AP-person': 51.22700060879134,\n",
       "               'AP-bicycle': 12.871287128712867,\n",
       "               'AP-car': 22.640482067001976,\n",
       "               'AP-motorcycle': 38.15055265061089,\n",
       "               'AP-airplane': 64.12932226362629,\n",
       "               'AP-bus': 54.867605009392015,\n",
       "               'AP-train': 53.04591977707602,\n",
       "               'AP-truck': 26.322510192195693,\n",
       "               'AP-boat': 16.155073460723536,\n",
       "               'AP-traffic light': 2.376237623762376,\n",
       "               'AP-fire hydrant': 69.28799912958328,\n",
       "               'AP-stop sign': 10.862800565770863,\n",
       "               'AP-parking meter': 0.0,\n",
       "               'AP-bench': 41.045162410977944,\n",
       "               'AP-bird': 27.792197076850545,\n",
       "               'AP-cat': 60.17121562459763,\n",
       "               'AP-dog': 33.594523491758046,\n",
       "               'AP-horse': 55.90844832276788,\n",
       "               'AP-sheep': 55.177562231631015,\n",
       "               'AP-cow': 38.93870180176472,\n",
       "               'AP-elephant': 52.14406381304275,\n",
       "               'AP-bear': 72.27929496246328,\n",
       "               'AP-zebra': 61.926592792494674,\n",
       "               'AP-giraffe': 75.72665809998367,\n",
       "               'AP-backpack': 0.0,\n",
       "               'AP-umbrella': 22.90274440873539,\n",
       "               'AP-handbag': 0.0,\n",
       "               'AP-tie': 0.0,\n",
       "               'AP-suitcase': 27.01488724723866,\n",
       "               'AP-frisbee': 46.32425742574257,\n",
       "               'AP-skis': 0.0,\n",
       "               'AP-snowboard': 23.29561527581329,\n",
       "               'AP-sports ball': 38.57673267326733,\n",
       "               'AP-kite': 44.342071262976624,\n",
       "               'AP-baseball bat': 10.940594059405939,\n",
       "               'AP-baseball glove': 0.0,\n",
       "               'AP-skateboard': 41.413141314131416,\n",
       "               'AP-surfboard': 30.198019801980198,\n",
       "               'AP-tennis racket': 55.80568519529269,\n",
       "               'AP-bottle': 40.47435260971903,\n",
       "               'AP-wine glass': 53.68186442704421,\n",
       "               'AP-cup': 34.66764588501562,\n",
       "               'AP-fork': 12.72041489863272,\n",
       "               'AP-knife': 7.194719471947194,\n",
       "               'AP-spoon': 17.762376237623762,\n",
       "               'AP-bowl': 10.90109138791117,\n",
       "               'AP-banana': 0.0,\n",
       "               'AP-apple': 29.719471947194716,\n",
       "               'AP-sandwich': 19.247524752475247,\n",
       "               'AP-orange': 58.44284428442844,\n",
       "               'AP-broccoli': 15.742574257425742,\n",
       "               'AP-carrot': 0.0,\n",
       "               'AP-hot dog': 39.65796579657965,\n",
       "               'AP-pizza': 39.56017295454111,\n",
       "               'AP-donut': 33.49074263131933,\n",
       "               'AP-cake': 39.68171841073262,\n",
       "               'AP-chair': 23.82159971407609,\n",
       "               'AP-couch': 25.87433358720488,\n",
       "               'AP-potted plant': 0.0,\n",
       "               'AP-bed': 26.891089108910894,\n",
       "               'AP-dining table': 3.5643564356435626,\n",
       "               'AP-toilet': 60.62621312136033,\n",
       "               'AP-tv': 59.9998678004581,\n",
       "               'AP-laptop': 61.03148758854856,\n",
       "               'AP-mouse': 28.91852289624567,\n",
       "               'AP-remote': 17.739038189533236,\n",
       "               'AP-keyboard': 41.72047740488335,\n",
       "               'AP-cell phone': 26.205013358478705,\n",
       "               'AP-microwave': 24.323432343234323,\n",
       "               'AP-oven': 29.306930693069305,\n",
       "               'AP-toaster': 0.0,\n",
       "               'AP-sink': 29.3475954738331,\n",
       "               'AP-refrigerator': 47.18564356435643,\n",
       "               'AP-book': 9.485148514851485,\n",
       "               'AP-clock': 43.63939826903327,\n",
       "               'AP-vase': 18.203120312031203,\n",
       "               'AP-scissors': 17.02970297029703,\n",
       "               'AP-teddy bear': 23.7018278750952,\n",
       "               'AP-hair drier': 0.0,\n",
       "               'AP-toothbrush': 45.44554455445544}),\n",
       "             ('segm',\n",
       "              {'AP': 29.30028996350929,\n",
       "               'AP50': 40.24024567160254,\n",
       "               'AP75': 33.45253068629018,\n",
       "               'APs': 9.615106009320085,\n",
       "               'APm': 26.34553365740682,\n",
       "               'APl': 38.25317433171843,\n",
       "               'AP-person': 45.78047840191369,\n",
       "               'AP-bicycle': 6.658415841584156,\n",
       "               'AP-car': 22.822608896912364,\n",
       "               'AP-motorcycle': 32.71082262072361,\n",
       "               'AP-airplane': 47.09842682093854,\n",
       "               'AP-bus': 55.887536258251025,\n",
       "               'AP-train': 54.30621585323697,\n",
       "               'AP-truck': 26.42339675143985,\n",
       "               'AP-boat': 19.628032675389445,\n",
       "               'AP-traffic light': 2.376237623762376,\n",
       "               'AP-fire hydrant': 65.983884102696,\n",
       "               'AP-stop sign': 24.66053748231966,\n",
       "               'AP-parking meter': 0.0,\n",
       "               'AP-bench': 36.720110920866524,\n",
       "               'AP-bird': 23.80254782997097,\n",
       "               'AP-cat': 68.68909188915013,\n",
       "               'AP-dog': 29.802882504999275,\n",
       "               'AP-horse': 42.03104144394124,\n",
       "               'AP-sheep': 49.82719232478621,\n",
       "               'AP-cow': 33.95066272883472,\n",
       "               'AP-elephant': 43.72999937739156,\n",
       "               'AP-bear': 72.37014470677838,\n",
       "               'AP-zebra': 48.521479014648676,\n",
       "               'AP-giraffe': 49.124649612241186,\n",
       "               'AP-backpack': 0.0,\n",
       "               'AP-umbrella': 29.16443798568553,\n",
       "               'AP-handbag': 0.0,\n",
       "               'AP-tie': 0.0,\n",
       "               'AP-suitcase': 26.497506406987448,\n",
       "               'AP-frisbee': 46.1469039761119,\n",
       "               'AP-skis': 0.0,\n",
       "               'AP-snowboard': 21.14285714285714,\n",
       "               'AP-sports ball': 38.31683168316832,\n",
       "               'AP-kite': 39.67446630749528,\n",
       "               'AP-baseball bat': 9.709570957095709,\n",
       "               'AP-baseball glove': 0.0,\n",
       "               'AP-skateboard': 32.31623162316232,\n",
       "               'AP-surfboard': 35.276402640264024,\n",
       "               'AP-tennis racket': 57.407967879160196,\n",
       "               'AP-bottle': 37.41163351300358,\n",
       "               'AP-wine glass': 48.177024469364234,\n",
       "               'AP-cup': 36.5319451259823,\n",
       "               'AP-fork': 7.956938550997957,\n",
       "               'AP-knife': 6.3743517208863745,\n",
       "               'AP-spoon': 17.82178217821782,\n",
       "               'AP-bowl': 11.083650564544948,\n",
       "               'AP-banana': 0.0,\n",
       "               'AP-apple': 31.73267326732673,\n",
       "               'AP-sandwich': 19.92079207920792,\n",
       "               'AP-orange': 57.1116111611161,\n",
       "               'AP-broccoli': 15.148514851485148,\n",
       "               'AP-carrot': 0.0,\n",
       "               'AP-hot dog': 39.43440058291544,\n",
       "               'AP-pizza': 40.01588963243181,\n",
       "               'AP-donut': 33.820688995929885,\n",
       "               'AP-cake': 41.07895483767201,\n",
       "               'AP-chair': 18.469984515205027,\n",
       "               'AP-couch': 26.50672759583651,\n",
       "               'AP-potted plant': 0.0,\n",
       "               'AP-bed': 26.6991984912777,\n",
       "               'AP-dining table': 1.5841584158415838,\n",
       "               'AP-toilet': 57.87867840274655,\n",
       "               'AP-tv': 64.0127852485527,\n",
       "               'AP-laptop': 59.91274076183193,\n",
       "               'AP-mouse': 45.04835422133034,\n",
       "               'AP-remote': 18.787128712871286,\n",
       "               'AP-keyboard': 47.45755825582558,\n",
       "               'AP-cell phone': 26.824611032531827,\n",
       "               'AP-microwave': 22.871287128712872,\n",
       "               'AP-oven': 25.379537953795378,\n",
       "               'AP-toaster': 0.0,\n",
       "               'AP-sink': 32.21534653465346,\n",
       "               'AP-refrigerator': 43.98349834983499,\n",
       "               'AP-book': 7.227722772277227,\n",
       "               'AP-clock': 43.7243872067636,\n",
       "               'AP-vase': 19.101967339591102,\n",
       "               'AP-scissors': 12.316831683168319,\n",
       "               'AP-teddy bear': 21.444229038288444,\n",
       "               'AP-hair drier': 0.0,\n",
       "               'AP-toothbrush': 40.39603960396039}),\n",
       "             ('visible',\n",
       "              {'AP': 31.20495233891193,\n",
       "               'AP50': 40.54257233435172,\n",
       "               'AP75': 35.95012163135272,\n",
       "               'APs': 10.30420433843021,\n",
       "               'APm': 27.729715211372557,\n",
       "               'APl': 40.87370930615567,\n",
       "               'AP-person': 48.43459892893903,\n",
       "               'AP-bicycle': 9.108910891089105,\n",
       "               'AP-car': 24.020682132836242,\n",
       "               'AP-motorcycle': 31.109193336916107,\n",
       "               'AP-airplane': 52.441892232423456,\n",
       "               'AP-bus': 58.601570866357086,\n",
       "               'AP-train': 57.1004663158162,\n",
       "               'AP-truck': 26.148919303695074,\n",
       "               'AP-boat': 18.453898237181267,\n",
       "               'AP-traffic light': 2.6732673267326734,\n",
       "               'AP-fire hydrant': 71.44271570014145,\n",
       "               'AP-stop sign': 27.213578500707218,\n",
       "               'AP-parking meter': 0.0,\n",
       "               'AP-bench': 32.16438373160624,\n",
       "               'AP-bird': 24.69259783121169,\n",
       "               'AP-cat': 68.24121271209216,\n",
       "               'AP-dog': 31.344144759303518,\n",
       "               'AP-horse': 44.48325863883771,\n",
       "               'AP-sheep': 55.76350572172619,\n",
       "               'AP-cow': 34.81703601135509,\n",
       "               'AP-elephant': 50.15251584682277,\n",
       "               'AP-bear': 82.17517136329018,\n",
       "               'AP-zebra': 54.642801644771374,\n",
       "               'AP-giraffe': 60.52348145031221,\n",
       "               'AP-backpack': 0.0,\n",
       "               'AP-umbrella': 31.222996103307565,\n",
       "               'AP-handbag': 0.0,\n",
       "               'AP-tie': 0.0,\n",
       "               'AP-suitcase': 27.50691230113723,\n",
       "               'AP-frisbee': 44.627859077116504,\n",
       "               'AP-skis': 0.0,\n",
       "               'AP-snowboard': 17.55304101838755,\n",
       "               'AP-sports ball': 38.5940594059406,\n",
       "               'AP-kite': 41.705988406541096,\n",
       "               'AP-baseball bat': 13.762376237623764,\n",
       "               'AP-baseball glove': 0.0,\n",
       "               'AP-skateboard': 36.26612661266127,\n",
       "               'AP-surfboard': 38.955895589558956,\n",
       "               'AP-tennis racket': 59.4107034589584,\n",
       "               'AP-bottle': 40.485263074651115,\n",
       "               'AP-wine glass': 52.91229987660421,\n",
       "               'AP-cup': 37.34304411043692,\n",
       "               'AP-fork': 8.04180418041804,\n",
       "               'AP-knife': 8.043375766148044,\n",
       "               'AP-spoon': 18.094059405940595,\n",
       "               'AP-bowl': 11.393355448076777,\n",
       "               'AP-banana': 0.0,\n",
       "               'AP-apple': 32.39273927392739,\n",
       "               'AP-sandwich': 20.442244224422442,\n",
       "               'AP-orange': 59.07215721572159,\n",
       "               'AP-broccoli': 15.148514851485148,\n",
       "               'AP-carrot': 0.0,\n",
       "               'AP-hot dog': 41.367565327961366,\n",
       "               'AP-pizza': 40.817678745285185,\n",
       "               'AP-donut': 36.68069198015621,\n",
       "               'AP-cake': 43.02063534112609,\n",
       "               'AP-chair': 19.575944173358632,\n",
       "               'AP-couch': 22.52932216298553,\n",
       "               'AP-potted plant': 0.0,\n",
       "               'AP-bed': 27.20933521923621,\n",
       "               'AP-dining table': 1.9801980198019795,\n",
       "               'AP-toilet': 62.38355708281451,\n",
       "               'AP-tv': 64.42989017341831,\n",
       "               'AP-laptop': 62.168481443131995,\n",
       "               'AP-mouse': 56.13470962480863,\n",
       "               'AP-remote': 19.207920792079207,\n",
       "               'AP-keyboard': 50.29917697652119,\n",
       "               'AP-cell phone': 31.2997728344263,\n",
       "               'AP-microwave': 38.052805280528055,\n",
       "               'AP-oven': 25.2970297029703,\n",
       "               'AP-toaster': 0.0,\n",
       "               'AP-sink': 33.02062706270627,\n",
       "               'AP-refrigerator': 51.66973125884018,\n",
       "               'AP-book': 10.376237623762377,\n",
       "               'AP-clock': 45.29208563236746,\n",
       "               'AP-vase': 20.822596545368825,\n",
       "               'AP-scissors': 11.524752475247526,\n",
       "               'AP-teddy bear': 22.114782906862114,\n",
       "               'AP-hair drier': 0.0,\n",
       "               'AP-toothbrush': 40.39603960396039})])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.DATASETS.TEST = (\"amodal_coco_val\",)\n",
    "cfg.DATASETS.TRAIN = (\"amodal_coco_train\",)\n",
    "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.9   # set the testing threshold for this model\n",
    "#  evaluate its performance using AP metric implemented in COCO API.\n",
    "from detectron2.evaluation import AmodalEvaluator, inference_on_dataset\n",
    "from detectron2.data import build_detection_test_loader\n",
    "evaluator = AmodalEvaluator(\"amodal_coco_val\", cfg, False, output_dir=\"myAmodalEvaluation\")\n",
    "val_loader = build_detection_test_loader(cfg, \"amodal_coco_val\")\n",
    "inference_on_dataset(trainer.model, val_loader, evaluator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.utils.visualizer import ColorMode\n",
    "import random\n",
    "from detectron2.data import DatasetCatalog\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.9   # set the testing threshold for this model\n",
    "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\n",
    "cfg.DATASETS.TEST = (\"amodal_coco_val\",)\n",
    "predictor = DefaultPredictor(cfg)\n",
    "dataset_dicts = DatasetCatalog.get(\"amodal_coco_val\")\n",
    "for d in random.sample(dataset_dicts, 1):    \n",
    "    im = cv2.imread(d[\"file_name\"])\n",
    "    visualizer = Visualizer(im[:, :, ::-1], metadata=MetadataCatalog.get(\"amodal_coco_val\"), scale=0.5)\n",
    "    vis = visualizer.draw_dataset_dict(d)\n",
    "    imshow(vis.get_image()[:, :, ::-1])\n",
    "#     import pdb;pdb.set_trace()\n",
    "    outputs = predictor(im)\n",
    "    v = Visualizer(im[:, :, ::-1],\n",
    "                   metadata=MetadataCatalog.get(\"amodal_coco_val\"), \n",
    "                   scale=0.8, \n",
    "                   instance_mode=ColorMode.IMAGE_BW   # remove the colors of unsegmented pixels\n",
    "    )\n",
    "    v = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
    "    imshow(v.get_image()[:, :, ::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
